{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "be348da5-5878-4298-b176-69d9ab23aa9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import idx2numpy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_images_path = 'dataset/train-images.idx3-ubyte'\n",
    "train_labels_path = 'dataset/train-labels.idx1-ubyte'\n",
    "test_images_path = 'dataset/t10k-images.idx3-ubyte'\n",
    "test_labels_path = 'dataset/t10k-labels.idx1-ubyte'\n",
    "\n",
    "# Carregar o conjunto de treino para avaliação\n",
    "arr_images_train = idx2numpy.convert_from_file(train_images_path)\n",
    "arr_labels_train = idx2numpy.convert_from_file(train_labels_path).astype(int)\n",
    "\n",
    "# Carregar o conjunto de teste para avaliação\n",
    "arr_images_test = idx2numpy.convert_from_file(test_images_path)\n",
    "arr_labels_test = idx2numpy.convert_from_file(test_labels_path).astype(int)\n",
    "\n",
    "example = np.asarray(arr_images_test[1]).squeeze()\n",
    "plt.imshow(example)\n",
    "\n",
    "print(arr_labels_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "31cc3563-6cd4-43db-8490-bc0f2cf331d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,)\n"
     ]
    }
   ],
   "source": [
    "print(arr_images_train.shape, arr_labels_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "259cba86-ea7d-4c6f-b452-47e44024b637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 785) (60000, 10)\n",
      "(10000, 785) (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Transformar as imagens em vetores e normalizá-las\n",
    "X_train = arr_images_train.reshape(arr_images_train.shape[0], -1)\n",
    "X_test = arr_images_test.reshape(arr_images_test.shape[0], -1)\n",
    "\n",
    "# Adicionar uma coluna de 1s para considerar o termo de bias (intercept) no modelo linear\n",
    "X_train = np.column_stack((X_train, np.ones(X_train.shape[0])))\n",
    "X_test = np.column_stack((X_test, np.ones(X_test.shape[0])))\n",
    "\n",
    "# Colocar y em formato de vetor (one hot)\n",
    "def one_hot_convert(vec):\n",
    "    matrix = []\n",
    "    for idx in vec:\n",
    "      m = np.zeros((10, 1))\n",
    "      m[idx] = 1\n",
    "      matrix.append(m)\n",
    "    return np.array(matrix)\n",
    "\n",
    "y_train = one_hot_convert(arr_labels_train).reshape(arr_labels_train.shape[0], -1)\n",
    "y_test = arr_labels_test.reshape(arr_labels_test.shape[0], -1)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "024193e4-b721-4bd4-b77c-9c0b63aabafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funções de ativação para o neurônio\n",
    "def activate_functions(type, matrix):\n",
    "    if type == 'sigmoid':\n",
    "        return 1 / (1 + np.exp(-matrix))\n",
    "    elif type == 'softmax':\n",
    "        exp_matrix = np.exp(matrix - np.max(matrix, axis=1, keepdims=True))\n",
    "        return exp_matrix / np.sum(exp_matrix, axis=1, keepdims=True)   \n",
    "    elif type == 'tanh':\n",
    "        return np.tanh(matrix)\n",
    "\n",
    "# Função de treino do MLP\n",
    "def mlp_train(X, y, n_neurons_hlayer, epochs, l_rate, criteria):\n",
    "    n_classes = y.shape[1]\n",
    "    n_features = X.shape[1]\n",
    "\n",
    "    w_input = np.random.randn(n_features, n_neurons_hlayer) * 0.1\n",
    "    w_output = np.random.randn(n_neurons_hlayer, n_classes) * 0.1\n",
    "\n",
    "    bias_input = np.random.randn(n_neurons_hlayer, 1) * 0.5\n",
    "    bias_output = np.random.randn(n_classes, 1) * 0.5\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # Forward\n",
    "        Zin = (X @ w_input) + bias_input.T\n",
    "        result_in = activate_functions('sigmoid', Zin)\n",
    "\n",
    "        Zout = (result_in @ w_output) + bias_output.T\n",
    "        result_out = activate_functions('softmax', Zout)\n",
    "\n",
    "        # Backpropagation\n",
    "        error_out = result_out - y\n",
    "        grad_out = error_out / len(X)\n",
    "\n",
    "        error_in = grad_out @ w_output.T\n",
    "        grad_in = error_in * result_in * (1 - result_in)\n",
    "\n",
    "        # Ajustar os pesos e os viéses\n",
    "        w_input -= l_rate * np.dot(X.T, grad_in)\n",
    "        w_output -= l_rate * np.dot(result_in.T, grad_out)\n",
    "\n",
    "        bias_input -= l_rate * np.sum(grad_in, axis=0, keepdims=True).T\n",
    "        bias_output -= l_rate * np.sum(grad_out, axis=0, keepdims=True).T\n",
    "\n",
    "        if epoch == 0:\n",
    "            loss = np.mean((y - result_out)**2)\n",
    "            print('Initial Epoch: {}, loss: {}'.format(epoch, loss))\n",
    "            best_loss = loss\n",
    "            \n",
    "        if epoch != 0 and epoch % 5 == 0:\n",
    "            loss = np.mean((y - result_out)**2)\n",
    "            print('Epoch: {}, loss: {}'.format(epoch, loss))\n",
    "\n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "                wait = 0\n",
    "            else: wait += 1\n",
    "\n",
    "            if wait >= criteria:\n",
    "                print('Final Epoch (loss stopped): {}, loss: {}'.format(epoch, loss))\n",
    "                return w_input, w_output, bias_input, bias_output\n",
    "\n",
    "    loss = np.mean((y - result_out)**2)\n",
    "    print('Last Epoch: {}, loss: {}'.format(epoch+1, loss))\n",
    "                                      \n",
    "    return w_input, w_output, bias_input, bias_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "74c4ace3-ca53-44c3-842e-f9b78c4e1720",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV 1/3] END ............hidden_layer_sizes=408;, score=0.860 total time=17.6min\n",
      "[CV 3/3] END .............hidden_layer_sizes=44;, score=0.775 total time= 1.8min\n",
      "[CV 2/3] END .............hidden_layer_sizes=74;, score=0.801 total time= 1.5min\n",
      "[CV 1/3] END ............hidden_layer_sizes=457;, score=0.837 total time= 9.0min\n",
      "[CV 1/3] END ............hidden_layer_sizes=112;, score=0.802 total time= 2.4min\n",
      "[CV 1/3] END ............hidden_layer_sizes=499;, score=0.842 total time=18.9min\n",
      "[CV 2/3] END .............hidden_layer_sizes=87;, score=0.808 total time= 1.9min\n",
      "[CV 3/3] END ............hidden_layer_sizes=408;, score=0.849 total time=12.4min\n",
      "[CV 3/3] END ............hidden_layer_sizes=402;, score=0.827 total time= 7.4min\n",
      "[CV 2/3] END ............hidden_layer_sizes=457;, score=0.863 total time=25.1min\n",
      "[CV 3/3] END ............hidden_layer_sizes=337;, score=0.826 total time= 6.2min\n",
      "[CV 3/3] END .............hidden_layer_sizes=87;, score=0.838 total time= 4.9min\n",
      "[CV 1/3] END ............hidden_layer_sizes=402;, score=0.811 total time= 7.6min\n",
      "[CV 2/3] END ............hidden_layer_sizes=402;, score=0.877 total time=18.4min\n",
      "[CV 2/3] END ............hidden_layer_sizes=112;, score=0.799 total time= 2.8min\n",
      "[CV 2/3] END ............hidden_layer_sizes=499;, score=0.816 total time= 7.8min\n",
      "[CV 1/3] END .............hidden_layer_sizes=14;, score=0.700 total time=  42.9s\n",
      "[CV 2/3] END .............hidden_layer_sizes=14;, score=0.135 total time=  38.3s\n",
      "[CV 3/3] END .............hidden_layer_sizes=14;, score=0.000 total time=  12.1s\n",
      "[CV 1/3] END ............hidden_layer_sizes=337;, score=0.841 total time=10.3min\n",
      "[CV 1/3] END .............hidden_layer_sizes=87;, score=0.743 total time=  56.0s\n",
      "[CV 2/3] END ............hidden_layer_sizes=408;, score=0.853 total time=15.0min\n",
      "[CV 1/3] END .............hidden_layer_sizes=44;, score=0.779 total time= 1.1min\n",
      "[CV 2/3] END .............hidden_layer_sizes=44;, score=0.815 total time= 1.2min\n",
      "[CV 1/3] END .............hidden_layer_sizes=74;, score=0.785 total time= 1.4min\n",
      "[CV 3/3] END .............hidden_layer_sizes=74;, score=0.829 total time= 3.3min\n",
      "[CV 3/3] END ............hidden_layer_sizes=457;, score=0.836 total time= 9.1min\n",
      "[CV 3/3] END ............hidden_layer_sizes=112;, score=0.819 total time= 2.5min\n",
      "[CV 3/3] END ............hidden_layer_sizes=499;, score=0.846 total time=10.3min\n",
      "[CV 2/3] END ............hidden_layer_sizes=337;, score=0.845 total time=10.3min\n",
      "Neurons in hidden layer:  408\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "def random_search(X, y):\n",
    "    mlp = MLPClassifier(activation='logistic', learning_rate_init=0.01) # Mantendo configurações do MLP - l_rate, função de ativação sigmoid, etc\n",
    "\n",
    "    params_search = {\"hidden_layer_sizes\": list(np.arange(2,500))}\n",
    "    \n",
    "    search = RandomizedSearchCV(mlp, param_distributions=params_search, n_jobs=-1, cv=3, verbose=5) # Busca do melhor numero de neuronios da camada\n",
    "    search.fit(X, y)\n",
    "    best = search.best_params_['hidden_layer_sizes']\n",
    "    \n",
    "    return best\n",
    "\n",
    "n_neurons = random_search(X_train, y_train)\n",
    "print(\"Neurons in hidden layer: \", n_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5d0cedb4-1c08-49bb-9e9b-f1c11dead62e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6471/752231831.py:4: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-matrix))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Epoch: 0, loss: 0.10557864528982763\n",
      "Epoch: 5, loss: 0.07356137036912765\n",
      "Epoch: 10, loss: 0.05763387692442999\n",
      "Epoch: 15, loss: 0.04798407035531647\n",
      "Epoch: 20, loss: 0.041787149781363636\n",
      "Epoch: 25, loss: 0.03754492663315067\n",
      "Epoch: 30, loss: 0.03444376521473046\n",
      "Epoch: 35, loss: 0.03200539821891574\n",
      "Epoch: 40, loss: 0.03008941360352721\n",
      "Epoch: 45, loss: 0.028479245406458024\n",
      "Epoch: 50, loss: 0.02716211428839557\n",
      "Epoch: 55, loss: 0.025995063261963564\n",
      "Epoch: 60, loss: 0.025024436251762348\n",
      "Epoch: 65, loss: 0.024135216018165973\n",
      "Epoch: 70, loss: 0.023405344576328722\n",
      "Epoch: 75, loss: 0.022700918232069205\n",
      "Epoch: 80, loss: 0.022121682911702707\n",
      "Epoch: 85, loss: 0.021547271320595824\n",
      "Epoch: 90, loss: 0.021077825156549792\n",
      "Epoch: 95, loss: 0.02056414883272983\n",
      "Epoch: 100, loss: 0.02016781890301798\n",
      "Epoch: 105, loss: 0.019750911056213988\n",
      "Epoch: 110, loss: 0.019413788649893817\n",
      "Epoch: 115, loss: 0.019037768509988857\n",
      "Epoch: 120, loss: 0.018752076508609224\n",
      "Epoch: 125, loss: 0.01842106568413397\n",
      "Epoch: 130, loss: 0.01817683172057907\n",
      "Epoch: 135, loss: 0.017872674466869545\n",
      "Epoch: 140, loss: 0.01765287713290758\n",
      "Epoch: 145, loss: 0.01737858226872329\n",
      "Epoch: 150, loss: 0.017180062757311695\n",
      "Epoch: 155, loss: 0.01693794373595417\n",
      "Epoch: 160, loss: 0.016758408251924584\n",
      "Epoch: 165, loss: 0.01654109940398638\n",
      "Epoch: 170, loss: 0.016371725273801117\n",
      "Epoch: 175, loss: 0.016166611436744256\n",
      "Epoch: 180, loss: 0.016018223672766268\n",
      "Epoch: 185, loss: 0.015833997128789436\n",
      "Epoch: 190, loss: 0.015677283876241288\n",
      "Epoch: 195, loss: 0.015505740246674607\n",
      "Epoch: 200, loss: 0.015384204213683339\n",
      "Epoch: 205, loss: 0.015222033942385332\n",
      "Epoch: 210, loss: 0.015100395823214518\n",
      "Epoch: 215, loss: 0.014944643938258098\n",
      "Epoch: 220, loss: 0.0148391580044068\n",
      "Epoch: 225, loss: 0.014696104556027665\n",
      "Epoch: 230, loss: 0.014593522143781623\n",
      "Epoch: 235, loss: 0.014441207812177809\n",
      "Epoch: 240, loss: 0.014357795604826576\n",
      "Epoch: 245, loss: 0.01423550008307765\n",
      "Epoch: 250, loss: 0.014143967741300887\n",
      "Epoch: 255, loss: 0.014005076376640651\n",
      "Epoch: 260, loss: 0.013939080941448847\n",
      "Epoch: 265, loss: 0.013825087093652289\n",
      "Epoch: 270, loss: 0.013744706248529434\n",
      "Epoch: 275, loss: 0.013628661082984076\n",
      "Epoch: 280, loss: 0.013554986203364141\n",
      "Epoch: 285, loss: 0.013426748714243176\n",
      "Epoch: 290, loss: 0.0133831458363227\n",
      "Epoch: 295, loss: 0.013281750043606282\n",
      "Epoch: 300, loss: 0.01319441099252872\n",
      "Epoch: 305, loss: 0.01310017278223308\n",
      "Epoch: 310, loss: 0.013046291021363702\n",
      "Epoch: 315, loss: 0.012932601039960193\n",
      "Epoch: 320, loss: 0.012874405073294123\n",
      "Epoch: 325, loss: 0.012787401083404456\n",
      "Epoch: 330, loss: 0.012744526944792677\n",
      "Epoch: 335, loss: 0.012660079517575825\n",
      "Epoch: 340, loss: 0.012606726512480388\n",
      "Epoch: 345, loss: 0.012490986215594078\n",
      "Epoch: 350, loss: 0.012448466494796882\n",
      "Epoch: 355, loss: 0.012383618047967318\n",
      "Epoch: 360, loss: 0.012383218479106416\n",
      "Epoch: 365, loss: 0.012252336620880974\n",
      "Epoch: 370, loss: 0.012182383584816204\n",
      "Epoch: 375, loss: 0.012095736980068461\n",
      "Epoch: 380, loss: 0.012096646611843993\n",
      "Epoch: 385, loss: 0.012021798091780413\n",
      "Epoch: 390, loss: 0.011981733935989376\n",
      "Epoch: 395, loss: 0.011888354173715247\n",
      "Epoch: 400, loss: 0.011859075823451172\n",
      "Epoch: 405, loss: 0.011789961182143162\n",
      "Epoch: 410, loss: 0.011747011776235229\n",
      "Epoch: 415, loss: 0.011673019684844356\n",
      "Epoch: 420, loss: 0.011623810204278879\n",
      "Epoch: 425, loss: 0.011564101952901302\n",
      "Epoch: 430, loss: 0.011561481247602472\n",
      "Epoch: 435, loss: 0.011477024783476144\n",
      "Epoch: 440, loss: 0.011451023698558506\n",
      "Epoch: 445, loss: 0.011346908282810302\n",
      "Epoch: 450, loss: 0.011336368108149553\n",
      "Epoch: 455, loss: 0.011304593305768754\n",
      "Epoch: 460, loss: 0.011305412592411943\n",
      "Epoch: 465, loss: 0.011176420391368977\n",
      "Epoch: 470, loss: 0.011118876349700041\n",
      "Epoch: 475, loss: 0.011078050113616348\n",
      "Epoch: 480, loss: 0.011124845987926553\n",
      "Epoch: 485, loss: 0.011064212950788873\n",
      "Epoch: 490, loss: 0.010982198442357026\n",
      "Epoch: 495, loss: 0.010888062293814827\n",
      "Epoch: 500, loss: 0.010890029053977056\n",
      "Epoch: 505, loss: 0.010865474916230059\n",
      "Epoch: 510, loss: 0.010837327127046716\n",
      "Epoch: 515, loss: 0.010749280050508816\n",
      "Epoch: 520, loss: 0.010724436546893841\n",
      "Epoch: 525, loss: 0.010686162724861186\n",
      "Epoch: 530, loss: 0.010683487981858992\n",
      "Epoch: 535, loss: 0.010624703814076654\n",
      "Epoch: 540, loss: 0.010608364552449949\n",
      "Epoch: 545, loss: 0.010513441560956432\n",
      "Epoch: 550, loss: 0.010484902646713383\n",
      "Epoch: 555, loss: 0.010461969138311805\n",
      "Epoch: 560, loss: 0.010454753269562814\n",
      "Epoch: 565, loss: 0.010398970997997463\n",
      "Epoch: 570, loss: 0.010372928963999783\n",
      "Epoch: 575, loss: 0.010331723893379276\n",
      "Epoch: 580, loss: 0.010315839986890797\n",
      "Epoch: 585, loss: 0.010246925529335002\n",
      "Epoch: 590, loss: 0.010205952243033216\n",
      "Epoch: 595, loss: 0.010175645722094286\n",
      "Epoch: 600, loss: 0.01019502260722039\n",
      "Epoch: 605, loss: 0.010168090926094274\n",
      "Epoch: 610, loss: 0.010116095005630798\n",
      "Epoch: 615, loss: 0.01001905387331515\n",
      "Epoch: 620, loss: 0.009993206764891474\n",
      "Epoch: 625, loss: 0.009997518619334022\n",
      "Epoch: 630, loss: 0.010013603672801484\n",
      "Epoch: 635, loss: 0.009943054366102416\n",
      "Epoch: 640, loss: 0.009905586965505983\n",
      "Epoch: 645, loss: 0.00985816900695633\n",
      "Epoch: 650, loss: 0.009878092170592222\n",
      "Epoch: 655, loss: 0.009826326375224967\n",
      "Epoch: 660, loss: 0.009786185620297727\n",
      "Epoch: 665, loss: 0.009738604792011973\n",
      "Epoch: 670, loss: 0.009710663026875479\n",
      "Epoch: 675, loss: 0.009717330067945906\n",
      "Epoch: 680, loss: 0.009725194713041849\n",
      "Epoch: 685, loss: 0.009674058898200907\n",
      "Epoch: 690, loss: 0.009622079933904739\n",
      "Epoch: 695, loss: 0.009560688587157574\n",
      "Epoch: 700, loss: 0.009560136881081392\n",
      "Epoch: 705, loss: 0.009519666514584755\n",
      "Epoch: 710, loss: 0.00952958299331899\n",
      "Epoch: 715, loss: 0.009524980556152028\n",
      "Epoch: 720, loss: 0.009521324940594449\n",
      "Epoch: 725, loss: 0.00944390524115034\n",
      "Epoch: 730, loss: 0.009388021555812882\n",
      "Epoch: 735, loss: 0.009340082631543612\n",
      "Epoch: 740, loss: 0.009330556622650841\n",
      "Epoch: 745, loss: 0.009358956830900685\n",
      "Epoch: 750, loss: 0.009381285430187906\n",
      "Epoch: 755, loss: 0.009322511967352154\n",
      "Epoch: 760, loss: 0.009283464486363307\n",
      "Epoch: 765, loss: 0.00921799591562808\n",
      "Epoch: 770, loss: 0.009208810454079131\n",
      "Epoch: 775, loss: 0.009160257981106326\n",
      "Epoch: 780, loss: 0.009178946668660067\n",
      "Epoch: 785, loss: 0.009170342870324037\n",
      "Epoch: 790, loss: 0.009139891991349361\n",
      "Epoch: 795, loss: 0.00908786570974234\n",
      "Epoch: 800, loss: 0.009086830217702692\n",
      "Epoch: 805, loss: 0.00904889627393568\n",
      "Epoch: 810, loss: 0.009038645027425967\n",
      "Epoch: 815, loss: 0.009024595386836429\n",
      "Epoch: 820, loss: 0.008974060076794434\n",
      "Epoch: 825, loss: 0.008947435325896147\n",
      "Epoch: 830, loss: 0.008919082448023976\n",
      "Epoch: 835, loss: 0.008901478522313941\n",
      "Epoch: 840, loss: 0.008916926652540227\n",
      "Epoch: 845, loss: 0.00890114391016858\n",
      "Epoch: 850, loss: 0.008896431472155762\n",
      "Epoch: 855, loss: 0.008861888958506628\n",
      "Epoch: 860, loss: 0.008826088583248124\n",
      "Epoch: 865, loss: 0.0087593667940579\n",
      "Epoch: 870, loss: 0.00874016022647599\n",
      "Epoch: 875, loss: 0.008727941118203908\n",
      "Epoch: 880, loss: 0.008759388277558969\n",
      "Epoch: 885, loss: 0.008742630642534832\n",
      "Epoch: 890, loss: 0.008770332419485823\n",
      "Epoch: 895, loss: 0.00869112381781765\n",
      "Epoch: 900, loss: 0.008628538152216732\n",
      "Epoch: 905, loss: 0.00859561202302872\n",
      "Epoch: 910, loss: 0.008603605262426936\n",
      "Epoch: 915, loss: 0.00862182714224013\n",
      "Epoch: 920, loss: 0.008658799592122741\n",
      "Epoch: 925, loss: 0.008574855316241355\n",
      "Epoch: 930, loss: 0.008555168710864447\n",
      "Epoch: 935, loss: 0.008508393188972093\n",
      "Epoch: 940, loss: 0.008489901073181802\n",
      "Epoch: 945, loss: 0.008461484138098784\n",
      "Epoch: 950, loss: 0.008488934278205566\n",
      "Epoch: 955, loss: 0.008475733354151197\n",
      "Epoch: 960, loss: 0.008475578549493691\n",
      "Epoch: 965, loss: 0.008429369154271109\n",
      "Epoch: 970, loss: 0.008408795899940223\n",
      "Epoch: 975, loss: 0.008378127929661958\n",
      "Epoch: 980, loss: 0.008349534318744306\n",
      "Epoch: 985, loss: 0.008334044169312414\n",
      "Epoch: 990, loss: 0.00833232734841711\n",
      "Epoch: 995, loss: 0.00832759431632017\n",
      "Epoch: 1000, loss: 0.008330490058152396\n",
      "Epoch: 1005, loss: 0.008298264841297975\n",
      "Epoch: 1010, loss: 0.008242546585627\n",
      "Epoch: 1015, loss: 0.008217045708112502\n",
      "Epoch: 1020, loss: 0.008221918777047225\n",
      "Epoch: 1025, loss: 0.008212757853404678\n",
      "Epoch: 1030, loss: 0.008221311534784857\n",
      "Epoch: 1035, loss: 0.008219598706654343\n",
      "Epoch: 1040, loss: 0.008208551253349848\n",
      "Epoch: 1045, loss: 0.008149397248902578\n",
      "Epoch: 1050, loss: 0.008147654729358331\n",
      "Epoch: 1055, loss: 0.00808941976028265\n",
      "Epoch: 1060, loss: 0.008094305221989988\n",
      "Epoch: 1065, loss: 0.00806607546783962\n",
      "Epoch: 1070, loss: 0.00807006468021108\n",
      "Epoch: 1075, loss: 0.00804902657953005\n",
      "Epoch: 1080, loss: 0.008048056480164085\n",
      "Epoch: 1085, loss: 0.00801778153227545\n",
      "Epoch: 1090, loss: 0.008006799708657173\n",
      "Epoch: 1095, loss: 0.007990575207292782\n",
      "Epoch: 1100, loss: 0.00799422013056389\n",
      "Epoch: 1105, loss: 0.007957021477520023\n",
      "Epoch: 1110, loss: 0.007956753509055153\n",
      "Epoch: 1115, loss: 0.007938130266729754\n",
      "Epoch: 1120, loss: 0.007937868488591479\n",
      "Epoch: 1125, loss: 0.00788805792578202\n",
      "Epoch: 1130, loss: 0.007868988861289063\n",
      "Epoch: 1135, loss: 0.007860337046557222\n",
      "Epoch: 1140, loss: 0.007881627242905883\n",
      "Epoch: 1145, loss: 0.007866205394314095\n",
      "Epoch: 1150, loss: 0.007868955532595253\n",
      "Epoch: 1155, loss: 0.007834400883794567\n",
      "Epoch: 1160, loss: 0.007836020041575078\n",
      "Epoch: 1165, loss: 0.007785967262603111\n",
      "Epoch: 1170, loss: 0.0077752000559773325\n",
      "Epoch: 1175, loss: 0.007789781362092866\n",
      "Epoch: 1180, loss: 0.007761829245126374\n",
      "Epoch: 1185, loss: 0.007722194743514869\n",
      "Epoch: 1190, loss: 0.007710255228392216\n",
      "Epoch: 1195, loss: 0.007697836944140501\n",
      "Epoch: 1200, loss: 0.007716055498125681\n",
      "Epoch: 1205, loss: 0.007701495771537348\n",
      "Epoch: 1210, loss: 0.0077015402724520425\n",
      "Epoch: 1215, loss: 0.0076865709199286664\n",
      "Epoch: 1220, loss: 0.007658780349383169\n",
      "Epoch: 1225, loss: 0.007629767513912027\n",
      "Epoch: 1230, loss: 0.007596603327109193\n",
      "Epoch: 1235, loss: 0.0075915373849861955\n",
      "Epoch: 1240, loss: 0.007598373828739545\n",
      "Epoch: 1245, loss: 0.007589784378152849\n",
      "Epoch: 1250, loss: 0.007603120320840077\n",
      "Epoch: 1255, loss: 0.007574554686500216\n",
      "Epoch: 1260, loss: 0.007584750644554858\n",
      "Epoch: 1265, loss: 0.0075300239887577695\n",
      "Epoch: 1270, loss: 0.007518687528150555\n",
      "Epoch: 1275, loss: 0.0074941438469168405\n",
      "Epoch: 1280, loss: 0.007469166469338946\n",
      "Epoch: 1285, loss: 0.007471582900587634\n",
      "Epoch: 1290, loss: 0.007452479831898201\n",
      "Epoch: 1295, loss: 0.007465553401038922\n",
      "Epoch: 1300, loss: 0.007474406199575001\n",
      "Epoch: 1305, loss: 0.007479865511108715\n",
      "Epoch: 1310, loss: 0.007448204356133142\n",
      "Epoch: 1315, loss: 0.007414987845821569\n",
      "Epoch: 1320, loss: 0.0073879680391533385\n",
      "Epoch: 1325, loss: 0.007367185834312901\n",
      "Epoch: 1330, loss: 0.007381619027748102\n",
      "Epoch: 1335, loss: 0.00736036299761671\n",
      "Epoch: 1340, loss: 0.007353017936240696\n",
      "Epoch: 1345, loss: 0.007357422812856972\n",
      "Epoch: 1350, loss: 0.007342393763065132\n",
      "Epoch: 1355, loss: 0.007345457025852533\n",
      "Epoch: 1360, loss: 0.007323852077504387\n",
      "Epoch: 1365, loss: 0.00729280262782907\n",
      "Epoch: 1370, loss: 0.007305327988418861\n",
      "Epoch: 1375, loss: 0.007276790016075438\n",
      "Epoch: 1380, loss: 0.007247806108364968\n",
      "Epoch: 1385, loss: 0.007249997293561502\n",
      "Epoch: 1390, loss: 0.007243146310577884\n",
      "Epoch: 1395, loss: 0.007236336222274604\n",
      "Epoch: 1400, loss: 0.007241521181860985\n",
      "Epoch: 1405, loss: 0.007201073689842978\n",
      "Epoch: 1410, loss: 0.00720900755109327\n",
      "Epoch: 1415, loss: 0.007197798974071446\n",
      "Epoch: 1420, loss: 0.007184744957568548\n",
      "Epoch: 1425, loss: 0.007165910315804025\n",
      "Epoch: 1430, loss: 0.007167125915977093\n",
      "Epoch: 1435, loss: 0.007154899536124949\n",
      "Epoch: 1440, loss: 0.007146036048520668\n",
      "Epoch: 1445, loss: 0.007126401827030678\n",
      "Epoch: 1450, loss: 0.007114582723711697\n",
      "Epoch: 1455, loss: 0.007105604480577916\n",
      "Epoch: 1460, loss: 0.00711126965912283\n",
      "Epoch: 1465, loss: 0.007094937059168081\n",
      "Epoch: 1470, loss: 0.007072216565498679\n",
      "Epoch: 1475, loss: 0.007046860006788765\n",
      "Epoch: 1480, loss: 0.007038277443629392\n",
      "Epoch: 1485, loss: 0.007021239718611951\n",
      "Epoch: 1490, loss: 0.007051936893773351\n",
      "Epoch: 1495, loss: 0.007020639660183858\n",
      "Epoch: 1500, loss: 0.0070383498654488065\n",
      "Epoch: 1505, loss: 0.0070185599784546035\n",
      "Epoch: 1510, loss: 0.007019976149095746\n",
      "Epoch: 1515, loss: 0.006993093407006931\n",
      "Epoch: 1520, loss: 0.006996102125947276\n",
      "Epoch: 1525, loss: 0.0069503732503298915\n",
      "Epoch: 1530, loss: 0.0069436092029437485\n",
      "Epoch: 1535, loss: 0.006934939724584643\n",
      "Epoch: 1540, loss: 0.006930066769841757\n",
      "Epoch: 1545, loss: 0.006917330257369866\n",
      "Epoch: 1550, loss: 0.00691811710392535\n",
      "Epoch: 1555, loss: 0.00690411019931169\n",
      "Epoch: 1560, loss: 0.006928686791950513\n",
      "Epoch: 1565, loss: 0.00689714613713978\n",
      "Epoch: 1570, loss: 0.0068692636544590594\n",
      "Epoch: 1575, loss: 0.006853534339813114\n",
      "Epoch: 1580, loss: 0.006846223137764382\n",
      "Epoch: 1585, loss: 0.006828764302050185\n",
      "Epoch: 1590, loss: 0.006838793205104138\n",
      "Epoch: 1595, loss: 0.006842063410580718\n",
      "Epoch: 1600, loss: 0.0068223028616411175\n",
      "Epoch: 1605, loss: 0.006809608161654029\n",
      "Epoch: 1610, loss: 0.006824858031186057\n",
      "Epoch: 1615, loss: 0.006805201339542562\n",
      "Epoch: 1620, loss: 0.006795631722247273\n",
      "Epoch: 1625, loss: 0.006773354453671446\n",
      "Epoch: 1630, loss: 0.006756348377651869\n",
      "Epoch: 1635, loss: 0.006734573717228317\n",
      "Epoch: 1640, loss: 0.006727705195432509\n",
      "Epoch: 1645, loss: 0.006718716553300045\n",
      "Epoch: 1650, loss: 0.006728797274175059\n",
      "Epoch: 1655, loss: 0.006720774267826594\n",
      "Epoch: 1660, loss: 0.006737274277662094\n",
      "Epoch: 1665, loss: 0.006730652045159886\n",
      "Epoch: 1670, loss: 0.006732220687738808\n",
      "Epoch: 1675, loss: 0.006707340572000623\n",
      "Epoch: 1680, loss: 0.006683063175681508\n",
      "Epoch: 1685, loss: 0.006661716123227192\n",
      "Epoch: 1690, loss: 0.006643452291476464\n",
      "Epoch: 1695, loss: 0.006627643775867673\n",
      "Epoch: 1700, loss: 0.006625471864200049\n",
      "Epoch: 1705, loss: 0.006626693215337438\n",
      "Epoch: 1710, loss: 0.006658944212583309\n",
      "Epoch: 1715, loss: 0.006620521792997688\n",
      "Epoch: 1720, loss: 0.0066056834082870155\n",
      "Epoch: 1725, loss: 0.006585246813226496\n",
      "Epoch: 1730, loss: 0.00659382644579081\n",
      "Epoch: 1735, loss: 0.006601206021255054\n",
      "Epoch: 1740, loss: 0.00659207446126855\n",
      "Epoch: 1745, loss: 0.006564196979259739\n",
      "Epoch: 1750, loss: 0.0065621236957541604\n",
      "Epoch: 1755, loss: 0.006556978687583289\n",
      "Epoch: 1760, loss: 0.0065339242032486575\n",
      "Epoch: 1765, loss: 0.006522600168012335\n",
      "Epoch: 1770, loss: 0.006521050092151594\n",
      "Epoch: 1775, loss: 0.006505348624449015\n",
      "Epoch: 1780, loss: 0.006517356378263025\n",
      "Epoch: 1785, loss: 0.006499592851202523\n",
      "Epoch: 1790, loss: 0.006510276615128615\n",
      "Epoch: 1795, loss: 0.0064938272841061325\n",
      "Epoch: 1800, loss: 0.006481352710971428\n",
      "Epoch: 1805, loss: 0.006451369875294594\n",
      "Epoch: 1810, loss: 0.006445562105208898\n",
      "Epoch: 1815, loss: 0.006447970453212288\n",
      "Epoch: 1820, loss: 0.006433942569448501\n",
      "Epoch: 1825, loss: 0.00645764039872053\n",
      "Epoch: 1830, loss: 0.006449920340917856\n",
      "Epoch: 1835, loss: 0.006434113994989833\n",
      "Epoch: 1840, loss: 0.006432895117354279\n",
      "Epoch: 1845, loss: 0.006408931892464799\n",
      "Epoch: 1850, loss: 0.006407420844218777\n",
      "Epoch: 1855, loss: 0.006387263713365272\n",
      "Epoch: 1860, loss: 0.006366206527971932\n",
      "Epoch: 1865, loss: 0.006352333309636274\n",
      "Epoch: 1870, loss: 0.006357969388688634\n",
      "Epoch: 1875, loss: 0.006347002219196504\n",
      "Epoch: 1880, loss: 0.006396607491575177\n",
      "Epoch: 1885, loss: 0.006365866231654495\n",
      "Epoch: 1890, loss: 0.006359717607839332\n",
      "Epoch: 1895, loss: 0.006318942421603404\n",
      "Epoch: 1900, loss: 0.006301532396499744\n",
      "Epoch: 1905, loss: 0.0063077465091739855\n",
      "Epoch: 1910, loss: 0.006298699525877207\n",
      "Epoch: 1915, loss: 0.0062970965617257435\n",
      "Epoch: 1920, loss: 0.006324767162096071\n",
      "Epoch: 1925, loss: 0.006328469188997369\n",
      "Epoch: 1930, loss: 0.006340414964491801\n",
      "Epoch: 1935, loss: 0.006289870588194777\n",
      "Epoch: 1940, loss: 0.006271005335265062\n",
      "Epoch: 1945, loss: 0.006238798883908627\n",
      "Epoch: 1950, loss: 0.006239829985578381\n",
      "Epoch: 1955, loss: 0.006236627954537589\n",
      "Epoch: 1960, loss: 0.006239835127095845\n",
      "Epoch: 1965, loss: 0.006236199281177333\n",
      "Epoch: 1970, loss: 0.006220797726763846\n",
      "Epoch: 1975, loss: 0.0062208426531265345\n",
      "Epoch: 1980, loss: 0.006199617057001097\n",
      "Epoch: 1985, loss: 0.006192985235332433\n",
      "Epoch: 1990, loss: 0.006189927808777467\n",
      "Epoch: 1995, loss: 0.006194849920062512\n",
      "Epoch: 2000, loss: 0.006183560024733771\n",
      "Epoch: 2005, loss: 0.006168157166707321\n",
      "Epoch: 2010, loss: 0.006168863723126483\n",
      "Epoch: 2015, loss: 0.006178995585799018\n",
      "Epoch: 2020, loss: 0.006186738256596165\n",
      "Epoch: 2025, loss: 0.006149792535132741\n",
      "Epoch: 2030, loss: 0.006143466493155017\n",
      "Epoch: 2035, loss: 0.006152189324023222\n",
      "Epoch: 2040, loss: 0.006141065584263011\n",
      "Epoch: 2045, loss: 0.006130373590466501\n",
      "Epoch: 2050, loss: 0.006098282633526368\n",
      "Epoch: 2055, loss: 0.00610534903998575\n",
      "Epoch: 2060, loss: 0.006090899196529554\n",
      "Epoch: 2065, loss: 0.006092915591583239\n",
      "Epoch: 2070, loss: 0.006088648819830406\n",
      "Epoch: 2075, loss: 0.006112570301864626\n",
      "Epoch: 2080, loss: 0.006110890648022693\n",
      "Epoch: 2085, loss: 0.00608010557299513\n",
      "Epoch: 2090, loss: 0.006065733885480888\n",
      "Epoch: 2095, loss: 0.006045486106601011\n",
      "Epoch: 2100, loss: 0.006051343647565663\n",
      "Epoch: 2105, loss: 0.006041172007973156\n",
      "Epoch: 2110, loss: 0.00604715049401905\n",
      "Epoch: 2115, loss: 0.006050891203966007\n",
      "Epoch: 2120, loss: 0.0060245650723269395\n",
      "Epoch: 2125, loss: 0.006006329114412238\n",
      "Epoch: 2130, loss: 0.006013872294840829\n",
      "Epoch: 2135, loss: 0.006020537310545354\n",
      "Epoch: 2140, loss: 0.005999492585880153\n",
      "Epoch: 2145, loss: 0.006001954030338614\n",
      "Epoch: 2150, loss: 0.006036324981161451\n",
      "Epoch: 2155, loss: 0.005999943588978571\n",
      "Epoch: 2160, loss: 0.005991967964554188\n",
      "Epoch: 2165, loss: 0.0059662331703727745\n",
      "Epoch: 2170, loss: 0.005949490643546997\n",
      "Epoch: 2175, loss: 0.0059397987397249614\n",
      "Epoch: 2180, loss: 0.005935314613118767\n",
      "Epoch: 2185, loss: 0.005955684716201103\n",
      "Epoch: 2190, loss: 0.0059381168060691\n",
      "Epoch: 2195, loss: 0.005930102619747082\n",
      "Epoch: 2200, loss: 0.005934703793354821\n",
      "Epoch: 2205, loss: 0.00590208704165304\n",
      "Epoch: 2210, loss: 0.005907956309709269\n",
      "Epoch: 2215, loss: 0.005911323967443836\n",
      "Epoch: 2220, loss: 0.005901693235754419\n",
      "Epoch: 2225, loss: 0.005887095831980168\n",
      "Epoch: 2230, loss: 0.005907575154797169\n",
      "Epoch: 2235, loss: 0.005874466037031038\n",
      "Epoch: 2240, loss: 0.005867521572897997\n",
      "Epoch: 2245, loss: 0.005858116237017209\n",
      "Epoch: 2250, loss: 0.005874773770484943\n",
      "Epoch: 2255, loss: 0.005843330838936481\n",
      "Epoch: 2260, loss: 0.0058445188039000315\n",
      "Epoch: 2265, loss: 0.005849702385821429\n",
      "Epoch: 2270, loss: 0.00584994369209549\n",
      "Epoch: 2275, loss: 0.005839148036600955\n",
      "Epoch: 2280, loss: 0.005842663580678785\n",
      "Epoch: 2285, loss: 0.005823716336542777\n",
      "Epoch: 2290, loss: 0.005841866027447582\n",
      "Epoch: 2295, loss: 0.00580436421380934\n",
      "Epoch: 2300, loss: 0.0058027070911738825\n",
      "Epoch: 2305, loss: 0.0058023883820152635\n",
      "Epoch: 2310, loss: 0.005804833535143132\n",
      "Epoch: 2315, loss: 0.005780831055640069\n",
      "Epoch: 2320, loss: 0.005789316448861759\n",
      "Epoch: 2325, loss: 0.005766505151830663\n",
      "Epoch: 2330, loss: 0.005757277811580934\n",
      "Epoch: 2335, loss: 0.0057562289337121115\n",
      "Epoch: 2340, loss: 0.005764749027325541\n",
      "Epoch: 2345, loss: 0.005740647717114938\n",
      "Epoch: 2350, loss: 0.005774025680821904\n",
      "Epoch: 2355, loss: 0.005752581381860444\n",
      "Epoch: 2360, loss: 0.005746324405932866\n",
      "Epoch: 2365, loss: 0.0057551808991213906\n",
      "Epoch: 2370, loss: 0.005732679291464331\n",
      "Epoch: 2375, loss: 0.005723564968606244\n",
      "Epoch: 2380, loss: 0.0057229300424927925\n",
      "Epoch: 2385, loss: 0.00570561773512128\n",
      "Epoch: 2390, loss: 0.005722821298799436\n",
      "Epoch: 2395, loss: 0.005701119202734798\n",
      "Epoch: 2400, loss: 0.00567936493484518\n",
      "Epoch: 2405, loss: 0.005694914050445074\n",
      "Epoch: 2410, loss: 0.005685345929229383\n",
      "Epoch: 2415, loss: 0.0056813335533804485\n",
      "Epoch: 2420, loss: 0.005685949532093906\n",
      "Epoch: 2425, loss: 0.005668517054268281\n",
      "Epoch: 2430, loss: 0.0056611157901434745\n",
      "Epoch: 2435, loss: 0.005659871618919861\n",
      "Epoch: 2440, loss: 0.005643343046537247\n",
      "Epoch: 2445, loss: 0.005658582432950253\n",
      "Epoch: 2450, loss: 0.0056529981657630415\n",
      "Epoch: 2455, loss: 0.005641587653280118\n",
      "Epoch: 2460, loss: 0.005622414282175225\n",
      "Epoch: 2465, loss: 0.005614300084510459\n",
      "Epoch: 2470, loss: 0.005607046603067854\n",
      "Epoch: 2475, loss: 0.005617235340086667\n",
      "Epoch: 2480, loss: 0.005607535691729746\n",
      "Epoch: 2485, loss: 0.005593646607836035\n",
      "Epoch: 2490, loss: 0.005587057176349636\n",
      "Epoch: 2495, loss: 0.005595834936495561\n",
      "Epoch: 2500, loss: 0.005581408928945013\n",
      "Epoch: 2505, loss: 0.005575261842709729\n",
      "Epoch: 2510, loss: 0.00557174261381801\n",
      "Epoch: 2515, loss: 0.005566916435734355\n",
      "Epoch: 2520, loss: 0.005571367458696957\n",
      "Epoch: 2525, loss: 0.005562923589117165\n",
      "Epoch: 2530, loss: 0.005563044202918642\n",
      "Epoch: 2535, loss: 0.005547329218725698\n",
      "Epoch: 2540, loss: 0.005547285114618641\n",
      "Epoch: 2545, loss: 0.005532237205084458\n",
      "Epoch: 2550, loss: 0.005548660404587906\n",
      "Epoch: 2555, loss: 0.005548794207431872\n",
      "Epoch: 2560, loss: 0.005533341856014327\n",
      "Epoch: 2565, loss: 0.0055205775384769085\n",
      "Epoch: 2570, loss: 0.005513763253728172\n",
      "Epoch: 2575, loss: 0.005503784410323738\n",
      "Epoch: 2580, loss: 0.005524755103760389\n",
      "Epoch: 2585, loss: 0.005497133454674048\n",
      "Epoch: 2590, loss: 0.005498319841006564\n",
      "Epoch: 2595, loss: 0.005495721162761344\n",
      "Epoch: 2600, loss: 0.005479457895961483\n",
      "Epoch: 2605, loss: 0.0054693794649850165\n",
      "Epoch: 2610, loss: 0.005483501880368744\n",
      "Epoch: 2615, loss: 0.00545576120842772\n",
      "Epoch: 2620, loss: 0.005450782873833754\n",
      "Epoch: 2625, loss: 0.005461632552917803\n",
      "Epoch: 2630, loss: 0.005474424581532065\n",
      "Epoch: 2635, loss: 0.005482086647917732\n",
      "Epoch: 2640, loss: 0.005455280063754768\n",
      "Epoch: 2645, loss: 0.005438052762812134\n",
      "Epoch: 2650, loss: 0.005428278994400723\n",
      "Epoch: 2655, loss: 0.005423666790833091\n",
      "Epoch: 2660, loss: 0.005418209071427202\n",
      "Epoch: 2665, loss: 0.0054140766129696805\n",
      "Epoch: 2670, loss: 0.0054352124476589195\n",
      "Epoch: 2675, loss: 0.005413993165683783\n",
      "Epoch: 2680, loss: 0.00541168404979114\n",
      "Epoch: 2685, loss: 0.005396104502115246\n",
      "Epoch: 2690, loss: 0.005398074030759389\n",
      "Epoch: 2695, loss: 0.005393592149680237\n",
      "Epoch: 2700, loss: 0.0053968354231821135\n",
      "Epoch: 2705, loss: 0.005387520453471132\n",
      "Epoch: 2710, loss: 0.005365877470823329\n",
      "Epoch: 2715, loss: 0.0053613260452140335\n",
      "Epoch: 2720, loss: 0.005384648502178238\n",
      "Epoch: 2725, loss: 0.005365641925823565\n",
      "Epoch: 2730, loss: 0.00537102978635656\n",
      "Epoch: 2735, loss: 0.00536008292417439\n",
      "Epoch: 2740, loss: 0.005348990240141826\n",
      "Epoch: 2745, loss: 0.0053299311132748725\n",
      "Epoch: 2750, loss: 0.005334887702034494\n",
      "Epoch: 2755, loss: 0.005332219859677442\n",
      "Epoch: 2760, loss: 0.0053426083058773135\n",
      "Epoch: 2765, loss: 0.005323316563265755\n",
      "Epoch: 2770, loss: 0.00530382416399255\n",
      "Epoch: 2775, loss: 0.005301542282036092\n",
      "Epoch: 2780, loss: 0.005320474298553404\n",
      "Epoch: 2785, loss: 0.005301218861396987\n",
      "Epoch: 2790, loss: 0.0053102434552382794\n",
      "Epoch: 2795, loss: 0.0052885806083448235\n",
      "Epoch: 2800, loss: 0.005285714580504989\n",
      "Epoch: 2805, loss: 0.005278205153929626\n",
      "Epoch: 2810, loss: 0.0052740918136803625\n",
      "Epoch: 2815, loss: 0.005281052924617695\n",
      "Epoch: 2820, loss: 0.005272492877625425\n",
      "Epoch: 2825, loss: 0.005266285384510471\n",
      "Epoch: 2830, loss: 0.005261883668522785\n",
      "Epoch: 2835, loss: 0.005276550978960252\n",
      "Epoch: 2840, loss: 0.0052601791493224045\n",
      "Epoch: 2845, loss: 0.005271155745927803\n",
      "Epoch: 2850, loss: 0.005247100013998906\n",
      "Epoch: 2855, loss: 0.005234784401863066\n",
      "Epoch: 2860, loss: 0.005243708274686044\n",
      "Epoch: 2865, loss: 0.005260685186132463\n",
      "Epoch: 2870, loss: 0.0052298709052278495\n",
      "Epoch: 2875, loss: 0.005213557078633456\n",
      "Epoch: 2880, loss: 0.00520846912707913\n",
      "Epoch: 2885, loss: 0.005204159465067101\n",
      "Epoch: 2890, loss: 0.005222275984775753\n",
      "Epoch: 2895, loss: 0.0052015027191282814\n",
      "Epoch: 2900, loss: 0.005220953793544421\n",
      "Epoch: 2905, loss: 0.005193929600420903\n",
      "Epoch: 2910, loss: 0.005200292822601062\n",
      "Epoch: 2915, loss: 0.005202508938750497\n",
      "Epoch: 2920, loss: 0.005202653842412722\n",
      "Epoch: 2925, loss: 0.005182146567302344\n",
      "Epoch: 2930, loss: 0.005174455671456718\n",
      "Epoch: 2935, loss: 0.005170894469534591\n",
      "Epoch: 2940, loss: 0.005157334012445553\n",
      "Epoch: 2945, loss: 0.00515386259803726\n",
      "Epoch: 2950, loss: 0.005145303581162906\n",
      "Epoch: 2955, loss: 0.005142479462551596\n",
      "Epoch: 2960, loss: 0.0051539950043556935\n",
      "Epoch: 2965, loss: 0.005146243127460053\n",
      "Epoch: 2970, loss: 0.005142416450021116\n",
      "Epoch: 2975, loss: 0.005133634069807636\n",
      "Epoch: 2980, loss: 0.0051360791918807535\n",
      "Epoch: 2985, loss: 0.005122052668734012\n",
      "Epoch: 2990, loss: 0.005131501723647711\n",
      "Epoch: 2995, loss: 0.005128589733359376\n",
      "Epoch: 3000, loss: 0.005126524728459897\n",
      "Epoch: 3005, loss: 0.005112584702279539\n",
      "Epoch: 3010, loss: 0.005108992013709851\n",
      "Epoch: 3015, loss: 0.005115759369817762\n",
      "Epoch: 3020, loss: 0.005096442884477452\n",
      "Epoch: 3025, loss: 0.005090189687866683\n",
      "Epoch: 3030, loss: 0.005084450464324403\n",
      "Epoch: 3035, loss: 0.005088922488060612\n",
      "Epoch: 3040, loss: 0.005086681011327075\n",
      "Epoch: 3045, loss: 0.0050801143857056785\n",
      "Epoch: 3050, loss: 0.005065367037473213\n",
      "Epoch: 3055, loss: 0.005063061332707661\n",
      "Epoch: 3060, loss: 0.0050575314914707865\n",
      "Epoch: 3065, loss: 0.00506540203663108\n",
      "Epoch: 3070, loss: 0.005070894209500755\n",
      "Epoch: 3075, loss: 0.005057638942561344\n",
      "Epoch: 3080, loss: 0.005055288112257021\n",
      "Epoch: 3085, loss: 0.0050508840243022185\n",
      "Epoch: 3090, loss: 0.0050465779332595035\n",
      "Epoch: 3095, loss: 0.005050455678764344\n",
      "Epoch: 3100, loss: 0.005042809010549667\n",
      "Epoch: 3105, loss: 0.005027681770367456\n",
      "Epoch: 3110, loss: 0.005016234430175617\n",
      "Epoch: 3115, loss: 0.005022808026347252\n",
      "Epoch: 3120, loss: 0.005014688718611793\n",
      "Epoch: 3125, loss: 0.005013137173248287\n",
      "Epoch: 3130, loss: 0.0050034631139546665\n",
      "Epoch: 3135, loss: 0.005010153669401582\n",
      "Epoch: 3140, loss: 0.005019002967171158\n",
      "Epoch: 3145, loss: 0.005010015224614871\n",
      "Epoch: 3150, loss: 0.004997533543164505\n",
      "Epoch: 3155, loss: 0.0049901700333992915\n",
      "Epoch: 3160, loss: 0.004985143848594457\n",
      "Epoch: 3165, loss: 0.004984390035519247\n",
      "Epoch: 3170, loss: 0.00498776912576963\n",
      "Epoch: 3175, loss: 0.004975156657903572\n",
      "Epoch: 3180, loss: 0.004965995810187117\n",
      "Epoch: 3185, loss: 0.004959437783290754\n",
      "Epoch: 3190, loss: 0.004967253892449296\n",
      "Epoch: 3195, loss: 0.004956181915652255\n",
      "Epoch: 3200, loss: 0.004950306994087797\n",
      "Epoch: 3205, loss: 0.004947324484771582\n",
      "Epoch: 3210, loss: 0.004953852024673301\n",
      "Epoch: 3215, loss: 0.004953592560742737\n",
      "Epoch: 3220, loss: 0.004943720987079402\n",
      "Epoch: 3225, loss: 0.004930795595755508\n",
      "Epoch: 3230, loss: 0.00492717668956029\n",
      "Epoch: 3235, loss: 0.0049333673845525314\n",
      "Epoch: 3240, loss: 0.004951772623604172\n",
      "Epoch: 3245, loss: 0.004926856915142671\n",
      "Epoch: 3250, loss: 0.004933549740163156\n",
      "Epoch: 3255, loss: 0.004912428577985327\n",
      "Epoch: 3260, loss: 0.004910218398766445\n",
      "Epoch: 3265, loss: 0.004902748890253491\n",
      "Epoch: 3270, loss: 0.004896781438894845\n",
      "Epoch: 3275, loss: 0.004892440935168076\n",
      "Epoch: 3280, loss: 0.004891732599232302\n",
      "Epoch: 3285, loss: 0.004894619634325307\n",
      "Epoch: 3290, loss: 0.0048910980586739\n",
      "Epoch: 3295, loss: 0.0048856024368606876\n",
      "Epoch: 3300, loss: 0.004892588169282653\n",
      "Epoch: 3305, loss: 0.0048926629449880365\n",
      "Epoch: 3310, loss: 0.0048780523165408846\n",
      "Epoch: 3315, loss: 0.00487271062989756\n",
      "Epoch: 3320, loss: 0.00487828741017277\n",
      "Epoch: 3325, loss: 0.004880794398954097\n",
      "Epoch: 3330, loss: 0.0048681551462106945\n",
      "Epoch: 3335, loss: 0.004855102315043667\n",
      "Epoch: 3340, loss: 0.004856267876980481\n",
      "Epoch: 3345, loss: 0.004850606432472208\n",
      "Epoch: 3350, loss: 0.004850262462263835\n",
      "Epoch: 3355, loss: 0.004842159635588448\n",
      "Epoch: 3360, loss: 0.004861671102008175\n",
      "Epoch: 3365, loss: 0.004834858760505082\n",
      "Epoch: 3370, loss: 0.004846184132861152\n",
      "Epoch: 3375, loss: 0.004836185493290275\n",
      "Epoch: 3380, loss: 0.004831798830825708\n",
      "Epoch: 3385, loss: 0.004820510663841274\n",
      "Epoch: 3390, loss: 0.004821811153453261\n",
      "Epoch: 3395, loss: 0.004825548446158373\n",
      "Epoch: 3400, loss: 0.004816835860383369\n",
      "Epoch: 3405, loss: 0.004807646510452024\n",
      "Epoch: 3410, loss: 0.004800011534557021\n",
      "Epoch: 3415, loss: 0.004796904007981206\n",
      "Epoch: 3420, loss: 0.004800316408903905\n",
      "Epoch: 3425, loss: 0.004796404506835244\n",
      "Epoch: 3430, loss: 0.0047848858970132805\n",
      "Epoch: 3435, loss: 0.004779455296872377\n",
      "Epoch: 3440, loss: 0.0047788089005647285\n",
      "Epoch: 3445, loss: 0.004776064347584703\n",
      "Epoch: 3450, loss: 0.004804634817883838\n",
      "Epoch: 3455, loss: 0.004767121990455977\n",
      "Epoch: 3460, loss: 0.004765746947080543\n",
      "Epoch: 3465, loss: 0.004782480904161456\n",
      "Epoch: 3470, loss: 0.004785015647312075\n",
      "Epoch: 3475, loss: 0.004777600276508066\n",
      "Epoch: 3480, loss: 0.004764308199357635\n",
      "Epoch: 3485, loss: 0.0047523784665330715\n",
      "Epoch: 3490, loss: 0.004749986976489833\n",
      "Epoch: 3495, loss: 0.004757797244229643\n",
      "Epoch: 3500, loss: 0.004745386138634953\n",
      "Epoch: 3505, loss: 0.004736732850736604\n",
      "Epoch: 3510, loss: 0.004735189048185733\n",
      "Epoch: 3515, loss: 0.004729349299898858\n",
      "Epoch: 3520, loss: 0.004724421195001324\n",
      "Epoch: 3525, loss: 0.004719354802410029\n",
      "Epoch: 3530, loss: 0.004719611371214716\n",
      "Epoch: 3535, loss: 0.0047223262156883515\n",
      "Epoch: 3540, loss: 0.004712680822587274\n",
      "Epoch: 3545, loss: 0.004725761860318676\n",
      "Epoch: 3550, loss: 0.0047200487786235\n",
      "Epoch: 3555, loss: 0.004713718958466139\n",
      "Epoch: 3560, loss: 0.004713437551343414\n",
      "Epoch: 3565, loss: 0.004695369715340062\n",
      "Epoch: 3570, loss: 0.0046901232372100745\n",
      "Epoch: 3575, loss: 0.004685114315673781\n",
      "Epoch: 3580, loss: 0.004682416296794772\n",
      "Epoch: 3585, loss: 0.004684771530955017\n",
      "Epoch: 3590, loss: 0.0046780764131055174\n",
      "Epoch: 3595, loss: 0.004688698896508333\n",
      "Epoch: 3600, loss: 0.004670087746674332\n",
      "Epoch: 3605, loss: 0.004665809175002691\n",
      "Epoch: 3610, loss: 0.004664420874083791\n",
      "Epoch: 3615, loss: 0.00465925395338494\n",
      "Epoch: 3620, loss: 0.004670647475132161\n",
      "Epoch: 3625, loss: 0.0046634376641255535\n",
      "Epoch: 3630, loss: 0.004661669636510037\n",
      "Epoch: 3635, loss: 0.004652006817826122\n",
      "Epoch: 3640, loss: 0.004666567423052594\n",
      "Epoch: 3645, loss: 0.0046565253576910486\n",
      "Epoch: 3650, loss: 0.004658473063833733\n",
      "Epoch: 3655, loss: 0.004644613541902715\n",
      "Epoch: 3660, loss: 0.004641857209640108\n",
      "Epoch: 3665, loss: 0.004648315068626057\n",
      "Epoch: 3670, loss: 0.004647373566671999\n",
      "Epoch: 3675, loss: 0.004629051802588138\n",
      "Epoch: 3680, loss: 0.004634212886284092\n",
      "Epoch: 3685, loss: 0.004614334260358794\n",
      "Epoch: 3690, loss: 0.004611911081082644\n",
      "Epoch: 3695, loss: 0.004608599384670498\n",
      "Epoch: 3700, loss: 0.004607937709248343\n",
      "Epoch: 3705, loss: 0.004604841300388756\n",
      "Epoch: 3710, loss: 0.00460994562733247\n",
      "Epoch: 3715, loss: 0.004616925307771432\n",
      "Epoch: 3720, loss: 0.0046050134666535975\n",
      "Epoch: 3725, loss: 0.004590640518337419\n",
      "Epoch: 3730, loss: 0.004584957959193779\n",
      "Epoch: 3735, loss: 0.004589107569709619\n",
      "Epoch: 3740, loss: 0.004584086592172344\n",
      "Epoch: 3745, loss: 0.004581974656392567\n",
      "Epoch: 3750, loss: 0.004578655045951516\n",
      "Epoch: 3755, loss: 0.0045719430830693155\n",
      "Epoch: 3760, loss: 0.0045720153323896865\n",
      "Epoch: 3765, loss: 0.00456840656180579\n",
      "Epoch: 3770, loss: 0.004561554282403377\n",
      "Epoch: 3775, loss: 0.004557888564521148\n",
      "Epoch: 3780, loss: 0.004573502799235513\n",
      "Epoch: 3785, loss: 0.00455623395261327\n",
      "Epoch: 3790, loss: 0.004557581041870889\n",
      "Epoch: 3795, loss: 0.0045538464399750385\n",
      "Epoch: 3800, loss: 0.004546247596558708\n",
      "Epoch: 3805, loss: 0.004543717200233269\n",
      "Epoch: 3810, loss: 0.004542455760636909\n",
      "Epoch: 3815, loss: 0.004540053756438546\n",
      "Epoch: 3820, loss: 0.0045451066838644374\n",
      "Epoch: 3825, loss: 0.004534922343432282\n",
      "Epoch: 3830, loss: 0.004538983837920562\n",
      "Epoch: 3835, loss: 0.004521751877770691\n",
      "Epoch: 3840, loss: 0.004527713224978175\n",
      "Epoch: 3845, loss: 0.004520459395289211\n",
      "Epoch: 3850, loss: 0.004517784497895461\n",
      "Epoch: 3855, loss: 0.004517677087478402\n",
      "Epoch: 3860, loss: 0.004520176365778149\n",
      "Epoch: 3865, loss: 0.004514767762389135\n",
      "Epoch: 3870, loss: 0.004509462101913522\n",
      "Epoch: 3875, loss: 0.004502422297477268\n",
      "Epoch: 3880, loss: 0.004497280013117589\n",
      "Epoch: 3885, loss: 0.0044914079265614675\n",
      "Epoch: 3890, loss: 0.00449123723353348\n",
      "Epoch: 3895, loss: 0.004489594478285878\n",
      "Epoch: 3900, loss: 0.004485651742145862\n",
      "Epoch: 3905, loss: 0.004481535824632628\n",
      "Epoch: 3910, loss: 0.004481858819921528\n",
      "Epoch: 3915, loss: 0.00447751559719859\n",
      "Epoch: 3920, loss: 0.004469953660039821\n",
      "Epoch: 3925, loss: 0.004466828454885792\n",
      "Epoch: 3930, loss: 0.004464833345119457\n",
      "Epoch: 3935, loss: 0.0044653529695271674\n",
      "Epoch: 3940, loss: 0.004475027492690224\n",
      "Epoch: 3945, loss: 0.0044684019958041036\n",
      "Epoch: 3950, loss: 0.004461138022296518\n",
      "Epoch: 3955, loss: 0.004456659576559342\n",
      "Epoch: 3960, loss: 0.004454609519490205\n",
      "Epoch: 3965, loss: 0.0044635072569004395\n",
      "Epoch: 3970, loss: 0.00444382723026081\n",
      "Epoch: 3975, loss: 0.004439105671993567\n",
      "Epoch: 3980, loss: 0.004434962082104634\n",
      "Epoch: 3985, loss: 0.004430812464589145\n",
      "Epoch: 3990, loss: 0.004427669001239331\n",
      "Epoch: 3995, loss: 0.00442492614601728\n",
      "Epoch: 4000, loss: 0.0044222253977782506\n",
      "Epoch: 4005, loss: 0.004419666655197646\n",
      "Epoch: 4010, loss: 0.00441959765891958\n",
      "Epoch: 4015, loss: 0.004449996238224451\n",
      "Epoch: 4020, loss: 0.004421293418894737\n",
      "Epoch: 4025, loss: 0.0044164263467875884\n",
      "Epoch: 4030, loss: 0.004421737527813218\n",
      "Epoch: 4035, loss: 0.004402610577092669\n",
      "Epoch: 4040, loss: 0.004399347634862722\n",
      "Epoch: 4045, loss: 0.004396703410544346\n",
      "Epoch: 4050, loss: 0.004392863208729337\n",
      "Epoch: 4055, loss: 0.004389553196130871\n",
      "Epoch: 4060, loss: 0.004386309319769086\n",
      "Epoch: 4065, loss: 0.004383163802074437\n",
      "Epoch: 4070, loss: 0.004380056826180074\n",
      "Epoch: 4075, loss: 0.004376819343014339\n",
      "Epoch: 4080, loss: 0.0043742545349443746\n",
      "Epoch: 4085, loss: 0.004374329778135415\n",
      "Epoch: 4090, loss: 0.004382874558641081\n",
      "Epoch: 4095, loss: 0.004371199447142864\n",
      "Epoch: 4100, loss: 0.004367380978845832\n",
      "Epoch: 4105, loss: 0.0043653431948583975\n",
      "Epoch: 4110, loss: 0.004360854486722488\n",
      "Epoch: 4115, loss: 0.004363702156883685\n",
      "Epoch: 4120, loss: 0.0043619815346165455\n",
      "Epoch: 4125, loss: 0.004363923946887669\n",
      "Epoch: 4130, loss: 0.004355054017936476\n",
      "Epoch: 4135, loss: 0.004346169696466132\n",
      "Epoch: 4140, loss: 0.004346361326643469\n",
      "Epoch: 4145, loss: 0.004339262977832482\n",
      "Epoch: 4150, loss: 0.004355492863993261\n",
      "Epoch: 4155, loss: 0.004332552303944134\n",
      "Epoch: 4160, loss: 0.004331023185069165\n",
      "Epoch: 4165, loss: 0.004340177692313123\n",
      "Epoch: 4170, loss: 0.004324884003647653\n",
      "Epoch: 4175, loss: 0.004326489217042133\n",
      "Epoch: 4180, loss: 0.004323187903190986\n",
      "Epoch: 4185, loss: 0.004315785279217315\n",
      "Epoch: 4190, loss: 0.004313323791428073\n",
      "Epoch: 4195, loss: 0.004320028424836283\n",
      "Epoch: 4200, loss: 0.004314874744336644\n",
      "Epoch: 4205, loss: 0.004305805475203089\n",
      "Epoch: 4210, loss: 0.0043018956003833195\n",
      "Epoch: 4215, loss: 0.004300135480222078\n",
      "Epoch: 4220, loss: 0.004302034763531308\n",
      "Epoch: 4225, loss: 0.0042954162823645205\n",
      "Epoch: 4230, loss: 0.004291062843569357\n",
      "Epoch: 4235, loss: 0.004289053726433383\n",
      "Epoch: 4240, loss: 0.004287798629357963\n",
      "Epoch: 4245, loss: 0.004287832450650416\n",
      "Epoch: 4250, loss: 0.004289567638931698\n",
      "Epoch: 4255, loss: 0.0042805132412401855\n",
      "Epoch: 4260, loss: 0.004275412160463879\n",
      "Epoch: 4265, loss: 0.004272430087673922\n",
      "Epoch: 4270, loss: 0.004269311113000063\n",
      "Epoch: 4275, loss: 0.004266423738263005\n",
      "Epoch: 4280, loss: 0.004262712972884531\n",
      "Epoch: 4285, loss: 0.004260239251174431\n",
      "Epoch: 4290, loss: 0.004262570063639423\n",
      "Epoch: 4295, loss: 0.004255099736605324\n",
      "Epoch: 4300, loss: 0.00425334925005833\n",
      "Epoch: 4305, loss: 0.004250758253785153\n",
      "Epoch: 4310, loss: 0.004249191290808446\n",
      "Epoch: 4315, loss: 0.004254595767812074\n",
      "Epoch: 4320, loss: 0.0042495013600000135\n",
      "Epoch: 4325, loss: 0.004248648088423209\n",
      "Epoch: 4330, loss: 0.004239473025615857\n",
      "Epoch: 4335, loss: 0.004241834172860343\n",
      "Epoch: 4340, loss: 0.004250203914979838\n",
      "Epoch: 4345, loss: 0.004239046990180759\n",
      "Epoch: 4350, loss: 0.004227736895968493\n",
      "Epoch: 4355, loss: 0.004225228110931975\n",
      "Epoch: 4360, loss: 0.004224719552919461\n",
      "Epoch: 4365, loss: 0.004219190492038641\n",
      "Epoch: 4370, loss: 0.0042164251218483155\n",
      "Epoch: 4375, loss: 0.004217211945294615\n",
      "Epoch: 4380, loss: 0.004213220704234138\n",
      "Epoch: 4385, loss: 0.004207550040227855\n",
      "Epoch: 4390, loss: 0.004204771452684156\n",
      "Epoch: 4395, loss: 0.004201992107366491\n",
      "Epoch: 4400, loss: 0.004199831647312158\n",
      "Epoch: 4405, loss: 0.004201131401989616\n",
      "Epoch: 4410, loss: 0.004196523937151968\n",
      "Epoch: 4415, loss: 0.004193487620310316\n",
      "Epoch: 4420, loss: 0.004191421773689819\n",
      "Epoch: 4425, loss: 0.004201164971226684\n",
      "Epoch: 4430, loss: 0.004185212949959168\n",
      "Epoch: 4435, loss: 0.004184286436873988\n",
      "Epoch: 4440, loss: 0.004179539240908648\n",
      "Epoch: 4445, loss: 0.004177529147945821\n",
      "Epoch: 4450, loss: 0.004189139885980195\n",
      "Epoch: 4455, loss: 0.004186843372801875\n",
      "Epoch: 4460, loss: 0.004185901708056481\n",
      "Epoch: 4465, loss: 0.004170174983677365\n",
      "Epoch: 4470, loss: 0.004167044735937372\n",
      "Epoch: 4475, loss: 0.004164318974520857\n",
      "Epoch: 4480, loss: 0.004169116890514069\n",
      "Epoch: 4485, loss: 0.004159386419850533\n",
      "Epoch: 4490, loss: 0.004155360585179678\n",
      "Epoch: 4495, loss: 0.004153007705300086\n",
      "Epoch: 4500, loss: 0.004157447911611148\n",
      "Epoch: 4505, loss: 0.004148360492725337\n",
      "Epoch: 4510, loss: 0.0041459325078983325\n",
      "Epoch: 4515, loss: 0.004143817065897725\n",
      "Epoch: 4520, loss: 0.0041558647331619925\n",
      "Epoch: 4525, loss: 0.004142147556794098\n",
      "Epoch: 4530, loss: 0.0041500830138832155\n",
      "Epoch: 4535, loss: 0.0041364432009954\n",
      "Epoch: 4540, loss: 0.0041460576683845654\n",
      "Epoch: 4545, loss: 0.004130149539497925\n",
      "Epoch: 4550, loss: 0.0041279439593460985\n",
      "Epoch: 4555, loss: 0.004124587766765914\n",
      "Epoch: 4560, loss: 0.00412590218349837\n",
      "Epoch: 4565, loss: 0.004118085570292976\n",
      "Epoch: 4570, loss: 0.0041169894445239824\n",
      "Epoch: 4575, loss: 0.004132088830035099\n",
      "Epoch: 4580, loss: 0.004114629459542095\n",
      "Epoch: 4585, loss: 0.004112072750860703\n",
      "Epoch: 4590, loss: 0.004110065125645029\n",
      "Epoch: 4595, loss: 0.004104814215818036\n",
      "Epoch: 4600, loss: 0.004103213516297623\n",
      "Epoch: 4605, loss: 0.004108690316376331\n",
      "Epoch: 4610, loss: 0.004100923132283984\n",
      "Epoch: 4615, loss: 0.004095089458834559\n",
      "Epoch: 4620, loss: 0.004093410744140086\n",
      "Epoch: 4625, loss: 0.004090405053998496\n",
      "Epoch: 4630, loss: 0.004087602634043614\n",
      "Epoch: 4635, loss: 0.004087045680599869\n",
      "Epoch: 4640, loss: 0.004082049902160107\n",
      "Epoch: 4645, loss: 0.004079748269212443\n",
      "Epoch: 4650, loss: 0.00407725116536639\n",
      "Epoch: 4655, loss: 0.004075149181520638\n",
      "Epoch: 4660, loss: 0.004077726853617924\n",
      "Epoch: 4665, loss: 0.004070910030641019\n",
      "Epoch: 4670, loss: 0.004069525636490643\n",
      "Epoch: 4675, loss: 0.004068353616908559\n",
      "Epoch: 4680, loss: 0.004064023056553076\n",
      "Epoch: 4685, loss: 0.004063914985683142\n",
      "Epoch: 4690, loss: 0.004069096546061655\n",
      "Epoch: 4695, loss: 0.004057816726033581\n",
      "Epoch: 4700, loss: 0.0040566973438869195\n",
      "Epoch: 4705, loss: 0.004059100655425967\n",
      "Epoch: 4710, loss: 0.004048560151678174\n",
      "Epoch: 4715, loss: 0.00404530691899406\n",
      "Epoch: 4720, loss: 0.004042569341129348\n",
      "Epoch: 4725, loss: 0.004040313234748681\n",
      "Epoch: 4730, loss: 0.004037672911186943\n",
      "Epoch: 4735, loss: 0.004035477586012192\n",
      "Epoch: 4740, loss: 0.004032862157157909\n",
      "Epoch: 4745, loss: 0.004030633068944774\n",
      "Epoch: 4750, loss: 0.004027877490978265\n",
      "Epoch: 4755, loss: 0.004025573268619451\n",
      "Epoch: 4760, loss: 0.004023040471030037\n",
      "Epoch: 4765, loss: 0.004021079715008047\n",
      "Epoch: 4770, loss: 0.004018831059502069\n",
      "Epoch: 4775, loss: 0.004025098434927192\n",
      "Epoch: 4780, loss: 0.004020570990884638\n",
      "Epoch: 4785, loss: 0.004017956584312996\n",
      "Epoch: 4790, loss: 0.004012655438778165\n",
      "Epoch: 4795, loss: 0.004010681468693511\n",
      "Epoch: 4800, loss: 0.004007362650569889\n",
      "Epoch: 4805, loss: 0.004014200289278626\n",
      "Epoch: 4810, loss: 0.0040074133142530815\n",
      "Epoch: 4815, loss: 0.004005327565337572\n",
      "Epoch: 4820, loss: 0.004012275591202992\n",
      "Epoch: 4825, loss: 0.004000960324975246\n",
      "Epoch: 4830, loss: 0.0039979139032673364\n",
      "Epoch: 4835, loss: 0.003993101176327539\n",
      "Epoch: 4840, loss: 0.003990828438310817\n",
      "Epoch: 4845, loss: 0.003992481715425766\n",
      "Epoch: 4850, loss: 0.003983553798909084\n",
      "Epoch: 4855, loss: 0.0039805213942502035\n",
      "Epoch: 4860, loss: 0.003978307835741867\n",
      "Epoch: 4865, loss: 0.003977599726418862\n",
      "Epoch: 4870, loss: 0.003976919446204924\n",
      "Epoch: 4875, loss: 0.003977507001138264\n",
      "Epoch: 4880, loss: 0.003971212865109342\n",
      "Epoch: 4885, loss: 0.0039683443083826375\n",
      "Epoch: 4890, loss: 0.0039733999997405576\n",
      "Epoch: 4895, loss: 0.003975200921804537\n",
      "Epoch: 4900, loss: 0.0039634043790316\n",
      "Epoch: 4905, loss: 0.003959844656559057\n",
      "Epoch: 4910, loss: 0.003962428809707619\n",
      "Epoch: 4915, loss: 0.0039654873755312\n",
      "Epoch: 4920, loss: 0.003959556042923829\n",
      "Epoch: 4925, loss: 0.0039533805426293775\n",
      "Epoch: 4930, loss: 0.003950171160665414\n",
      "Epoch: 4935, loss: 0.003947975592904915\n",
      "Epoch: 4940, loss: 0.0039449270134838555\n",
      "Epoch: 4945, loss: 0.003948479358475823\n",
      "Epoch: 4950, loss: 0.003940230320347646\n",
      "Epoch: 4955, loss: 0.003937880784016351\n",
      "Epoch: 4960, loss: 0.003935640852492351\n",
      "Epoch: 4965, loss: 0.00393339867669171\n",
      "Epoch: 4970, loss: 0.003931249629124669\n",
      "Epoch: 4975, loss: 0.003929269745640108\n",
      "Epoch: 4980, loss: 0.003931584623376227\n",
      "Epoch: 4985, loss: 0.003927725998331908\n",
      "Epoch: 4990, loss: 0.003933990014287177\n",
      "Epoch: 4995, loss: 0.00392437329582397\n",
      "Last Epoch: 5000, loss: 0.003920591465465176\n"
     ]
    }
   ],
   "source": [
    "w_input, w_output, bias_input, bias_output = mlp_train(X_train, y_train, n_neurons, epochs=5000, l_rate=0.1, criteria=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0cb33864-4d8c-459e-8c54-0b9eb2380d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,)\n",
      "(10000, 1)\n",
      "(10000, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6471/752231831.py:4: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-matrix))\n"
     ]
    }
   ],
   "source": [
    "# Função de predição do MLP\n",
    "def mlp_predict(X, w_in, w_out, bias_in, bias_out):\n",
    "    # Forward\n",
    "    Zin = (X @ w_in) + bias_in.T\n",
    "    result_in = activate_functions('sigmoid', Zin)\n",
    "\n",
    "    Zout = (result_in @ w_out) + bias_out.T\n",
    "    result_out = activate_functions('softmax', Zout)\n",
    "\n",
    "    # Converte as saídas para as classes preditas (0 a 9) usando a função argmax\n",
    "    # A classe predita será o índice do valor máximo em cada linha\n",
    "    print(result_out)\n",
    "    classe = np.argmax(result_out, axis=1)\n",
    "    print(classe.shape)\n",
    "\n",
    "    return np.expand_dims(classe, axis=1)\n",
    "\n",
    "# Realizar a predição no conjunto de teste\n",
    "y_pred_test = mlp_predict(X_test, w_input, w_output, bias_input, bias_output)\n",
    "\n",
    "print(y_pred_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bf44b347-dfab-4dd4-a3c8-b4b59da9908c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate: 0.0684\n"
     ]
    }
   ],
   "source": [
    "# Avaliar o desempenho do classificador\n",
    "error = (10000 - sum(y_pred_test == y_test)) / 10000 \n",
    "print(\"Error rate: {}\".format(error[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4dda21-8ac4-4888-9805-0e12c3de1531",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": ".env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
