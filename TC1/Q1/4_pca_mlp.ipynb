{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be348da5-5878-4298-b176-69d9ab23aa9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import idx2numpy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_images_path = 'dataset/train-images.idx3-ubyte'\n",
    "train_labels_path = 'dataset/train-labels.idx1-ubyte'\n",
    "test_images_path = 'dataset/t10k-images.idx3-ubyte'\n",
    "test_labels_path = 'dataset/t10k-labels.idx1-ubyte'\n",
    "\n",
    "# Carregar o conjunto de treino para avaliação\n",
    "arr_images_train = idx2numpy.convert_from_file(train_images_path)\n",
    "arr_labels_train = idx2numpy.convert_from_file(train_labels_path).astype(int)\n",
    "\n",
    "# Carregar o conjunto de teste para avaliação\n",
    "arr_images_test = idx2numpy.convert_from_file(test_images_path)\n",
    "arr_labels_test = idx2numpy.convert_from_file(test_labels_path).astype(int)\n",
    "\n",
    "example = np.asarray(arr_images_test[1]).squeeze()\n",
    "plt.imshow(example)\n",
    "\n",
    "print(arr_labels_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31cc3563-6cd4-43db-8490-bc0f2cf331d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,)\n"
     ]
    }
   ],
   "source": [
    "print(arr_images_train.shape, arr_labels_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "259cba86-ea7d-4c6f-b452-47e44024b637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 785) (60000, 10)\n",
      "(10000, 785) (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Transformar as imagens em vetores e normalizá-las\n",
    "X_train = arr_images_train.reshape(arr_images_train.shape[0], -1)\n",
    "X_test = arr_images_test.reshape(arr_images_test.shape[0], -1)\n",
    "\n",
    "# Adicionar uma coluna de 1s para considerar o termo de bias (intercept) no modelo linear\n",
    "X_train = np.column_stack((X_train, np.ones(X_train.shape[0])))\n",
    "X_test = np.column_stack((X_test, np.ones(X_test.shape[0])))\n",
    "\n",
    "# Colocar y em formato de vetor (one hot)\n",
    "def one_hot_convert(vec):\n",
    "    matrix = []\n",
    "    for idx in vec:\n",
    "      m = np.zeros((10, 1))\n",
    "      m[idx] = 1\n",
    "      matrix.append(m)\n",
    "    return np.array(matrix)\n",
    "\n",
    "y_train = one_hot_convert(arr_labels_train).reshape(arr_labels_train.shape[0], -1)\n",
    "y_test = arr_labels_test.reshape(arr_labels_test.shape[0], -1)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d66497db-8ef6-41ab-bd3e-4fdff0c35973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(785, 235)\n"
     ]
    }
   ],
   "source": [
    "# Função para definir a matriz de transformação\n",
    "def pca_train(X, tol):\n",
    "    n = X.shape[0]\n",
    "\n",
    "    # Normalização dos dados\n",
    "    std = np.std(X, axis=0)\n",
    "    X_norm = (X - np.mean(X, axis=0)) / np.where(std == 0, 1e-9, std) # Caso divisão por zero\n",
    "\n",
    "    # Matriz de covariância\n",
    "    Cx = (X_norm.T @ X_norm) / n\n",
    "    \n",
    "    eigenvalues, eigenvectors = np.linalg.eig(Cx)\n",
    "    \n",
    "    idxs = eigenvalues.argsort()[::-1] # Índices do maior para o menor\n",
    "    eigenvalues = np.sort(eigenvalues)[::-1]\n",
    "    eigenvectors = eigenvectors[:, idxs] # Montagem de autovetores\n",
    "\n",
    "    var = np.cumsum(eigenvalues) / np.sum(eigenvalues) # variancia explicada\n",
    "\n",
    "    num_components = np.sum(var <= tol) # Num. compon. principais\n",
    "\n",
    "    Vq = eigenvectors[:, :num_components] # Matriz com q primeiros autovetores\n",
    "\n",
    "    return Vq\n",
    "\n",
    "# Função para aplicar a transformação nos dados\n",
    "def pca_transform(Vq, X):\n",
    "    # Normalização dos dados\n",
    "    std = np.std(X, axis=0)\n",
    "    X_norm = (X - np.mean(X, axis=0)) / np.where(std == 0, 1e-9, std) # Caso divisão por zero\n",
    "    \n",
    "    Z = X_norm @ Vq # Transformação\n",
    "    \n",
    "    return Z\n",
    "\n",
    "Vq = pca_train(X_train, 0.9)\n",
    "print(Vq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f684c3e4-ce10-4cc2-b95e-0f26c5eda69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 235) (10000, 235)\n"
     ]
    }
   ],
   "source": [
    "X_train = pca_transform(Vq, X_train)\n",
    "X_test = pca_transform(Vq, X_test)\n",
    "\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "024193e4-b721-4bd4-b77c-9c0b63aabafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funções de ativação para o neurônio\n",
    "def activate_functions(type, matrix):\n",
    "    if type == 'sigmoid':\n",
    "        return 1 / (1 + np.exp(-matrix))\n",
    "    elif type == 'softmax':\n",
    "        exp_matrix = np.exp(matrix - np.max(matrix, axis=1, keepdims=True))\n",
    "        return exp_matrix / np.sum(exp_matrix, axis=1, keepdims=True)   \n",
    "    elif type == 'tanh':\n",
    "        return np.tanh(matrix)\n",
    "\n",
    "# Função de treino do MLP\n",
    "def mlp_train(X, y, n_neurons_hlayer, epochs, l_rate, criteria):\n",
    "    n_classes = y.shape[1]\n",
    "    n_features = X.shape[1]\n",
    "\n",
    "    w_input = np.random.randn(n_features, n_neurons_hlayer) * 0.1\n",
    "    w_output = np.random.randn(n_neurons_hlayer, n_classes) * 0.1\n",
    "\n",
    "    bias_input = np.random.randn(n_neurons_hlayer, 1) * 0.5\n",
    "    bias_output = np.random.randn(n_classes, 1) * 0.5\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # Forward\n",
    "        Zin = (X @ w_input) + bias_input.T\n",
    "        result_in = activate_functions('sigmoid', Zin)\n",
    "\n",
    "        Zout = (result_in @ w_output) + bias_output.T\n",
    "        result_out = activate_functions('softmax', Zout)\n",
    "\n",
    "        # Backpropagation\n",
    "        error_out = result_out - y\n",
    "        grad_out = error_out / len(X)\n",
    "\n",
    "        error_in = grad_out @ w_output.T\n",
    "        grad_in = error_in * result_in * (1 - result_in)\n",
    "\n",
    "        # Ajustar os pesos e os viéses\n",
    "        w_input -= l_rate * np.dot(X.T, grad_in)\n",
    "        w_output -= l_rate * np.dot(result_in.T, grad_out)\n",
    "\n",
    "        bias_input -= l_rate * np.sum(grad_in, axis=0, keepdims=True).T\n",
    "        bias_output -= l_rate * np.sum(grad_out, axis=0, keepdims=True).T\n",
    "\n",
    "        if epoch == 0:\n",
    "            loss = np.mean((y - result_out)**2)\n",
    "            print('Initial Epoch: {}, loss: {}'.format(epoch, loss))\n",
    "            best_loss = loss\n",
    "            \n",
    "        if epoch != 0 and epoch % 5 == 0:\n",
    "            loss = np.mean((y - result_out)**2)\n",
    "            print('Epoch: {}, loss: {}'.format(epoch, loss))\n",
    "\n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "                wait = 0\n",
    "            else: wait += 1\n",
    "\n",
    "            if wait >= criteria:\n",
    "                print('Final Epoch (loss stopped): {}, loss: {}'.format(epoch, loss))\n",
    "                return w_input, w_output, bias_input, bias_output\n",
    "\n",
    "    loss = np.mean((y - result_out)**2)\n",
    "    print('Last Epoch: {}, loss: {}'.format(epoch+1, loss))\n",
    "                                      \n",
    "    return w_input, w_output, bias_input, bias_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74c4ace3-ca53-44c3-842e-f9b78c4e1720",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Neurons in hidden layer:  439\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "def random_search(X, y):\n",
    "    mlp = MLPClassifier(activation='logistic', learning_rate_init=0.01) # Mantendo configurações do MLP - l_rate, função de ativação sigmoid, etc\n",
    "\n",
    "    params_search = {\"hidden_layer_sizes\": list(np.arange(2,500))}\n",
    "    \n",
    "    search = RandomizedSearchCV(mlp, param_distributions=params_search, n_jobs=-1, cv=3, verbose=5) # Busca do melhor numero de neuronios da camada\n",
    "    search.fit(X, y)\n",
    "    best = search.best_params_['hidden_layer_sizes']\n",
    "    \n",
    "    return best\n",
    "\n",
    "n_neurons = random_search(X_train, y_train)\n",
    "print(\"Neurons in hidden layer: \", n_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d0cedb4-1c08-49bb-9e9b-f1c11dead62e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Epoch: 0, loss: 0.12413031877024133\n",
      "Epoch: 5, loss: 0.07880338246478479\n",
      "Epoch: 10, loss: 0.06440468003221897\n",
      "Epoch: 15, loss: 0.053959957125631\n",
      "Epoch: 20, loss: 0.04659160132963804\n",
      "Epoch: 25, loss: 0.0413209805037933\n",
      "Epoch: 30, loss: 0.037449957637020435\n",
      "Epoch: 35, loss: 0.03452008824956472\n",
      "Epoch: 40, loss: 0.032236535362526235\n",
      "Epoch: 45, loss: 0.030408932656680883\n",
      "Epoch: 50, loss: 0.028912176432550306\n",
      "Epoch: 55, loss: 0.027662033397503175\n",
      "Epoch: 60, loss: 0.026600275118227955\n",
      "Epoch: 65, loss: 0.025685592242437733\n",
      "Epoch: 70, loss: 0.024887957005043423\n",
      "Epoch: 75, loss: 0.024185051710184074\n",
      "Epoch: 80, loss: 0.023559953359508924\n",
      "Epoch: 85, loss: 0.022999597237009804\n",
      "Epoch: 90, loss: 0.02249373413811868\n",
      "Epoch: 95, loss: 0.022034207414823266\n",
      "Epoch: 100, loss: 0.0216144417290828\n",
      "Epoch: 105, loss: 0.021229074855856078\n",
      "[CV 3/3] END ............hidden_layer_sizes=469;, score=0.939 total time= 2.7min\n",
      "[CV 3/3] END ............hidden_layer_sizes=342;, score=0.933 total time= 1.6min\n",
      "[CV 3/3] END ............hidden_layer_sizes=436;, score=0.936 total time= 2.0min\n",
      "[CV 2/3] END ............hidden_layer_sizes=439;, score=0.936 total time= 2.0min\n",
      "[CV 2/3] END .............hidden_layer_sizes=69;, score=0.904 total time=  48.0s\n",
      "[CV 3/3] END ............hidden_layer_sizes=119;, score=0.918 total time=  47.7s\n",
      "[CV 1/3] END ............hidden_layer_sizes=174;, score=0.923 total time= 1.1min\n",
      "Epoch: 110, loss: 0.020873688025146508\n",
      "[CV 2/3] END ............hidden_layer_sizes=469;, score=0.936 total time= 2.9min\n",
      "[CV 1/3] END ............hidden_layer_sizes=243;, score=0.930 total time= 1.2min\n",
      "[CV 1/3] END ............hidden_layer_sizes=436;, score=0.936 total time= 2.1min\n",
      "[CV 2/3] END .............hidden_layer_sizes=83;, score=0.901 total time=  39.8s\n",
      "[CV 3/3] END ............hidden_layer_sizes=439;, score=0.938 total time= 2.0min\n",
      "[CV 2/3] END ............hidden_layer_sizes=119;, score=0.912 total time= 1.1min\n",
      "[CV 3/3] END .............hidden_layer_sizes=42;, score=0.884 total time= 1.3min\n",
      "Epoch: 115, loss: 0.020544605372419398\n",
      "Epoch: 120, loss: 0.020238742670780553\n",
      "Epoch: 125, loss: 0.019953491751946852\n",
      "[CV 1/3] END ............hidden_layer_sizes=342;, score=0.934 total time= 2.1min\n",
      "[CV 2/3] END ............hidden_layer_sizes=342;, score=0.935 total time= 1.8min\n",
      "[CV 3/3] END ............hidden_layer_sizes=243;, score=0.930 total time= 1.3min\n",
      "[CV 1/3] END .............hidden_layer_sizes=83;, score=0.906 total time= 1.2min\n",
      "[CV 1/3] END ............hidden_layer_sizes=439;, score=0.939 total time= 2.2min\n",
      "[CV 3/3] END .............hidden_layer_sizes=69;, score=0.899 total time= 1.2min\n",
      "[CV 2/3] END .............hidden_layer_sizes=42;, score=0.882 total time= 1.2min\n",
      "[CV 2/3] END ............hidden_layer_sizes=174;, score=0.923 total time=  42.3s\n",
      "[CV 1/3] END ............hidden_layer_sizes=469;, score=0.936 total time= 2.9min\n",
      "[CV 2/3] END ............hidden_layer_sizes=243;, score=0.927 total time= 1.3min\n",
      "[CV 2/3] END ............hidden_layer_sizes=436;, score=0.935 total time= 2.1min\n",
      "[CV 3/3] END .............hidden_layer_sizes=83;, score=0.903 total time= 1.5min\n",
      "[CV 1/3] END .............hidden_layer_sizes=69;, score=0.900 total time= 1.0min\n",
      "[CV 1/3] END ............hidden_layer_sizes=119;, score=0.917 total time=  53.9s\n",
      "[CV 1/3] END .............hidden_layer_sizes=42;, score=0.889 total time= 1.2min\n",
      "[CV 3/3] END ............hidden_layer_sizes=174;, score=0.925 total time=  43.9s\n",
      "Epoch: 130, loss: 0.019686631142799556\n",
      "Epoch: 135, loss: 0.019436256213855477\n",
      "Epoch: 140, loss: 0.019200724027916862\n",
      "Epoch: 145, loss: 0.018978609389113256\n",
      "Epoch: 150, loss: 0.018768669515186798\n",
      "Epoch: 155, loss: 0.018569815413325603\n",
      "Epoch: 160, loss: 0.018381088514191523\n",
      "Epoch: 165, loss: 0.01820164146508359\n",
      "Epoch: 170, loss: 0.018030722238859473\n",
      "Epoch: 175, loss: 0.017867660906018677\n",
      "Epoch: 180, loss: 0.017711858561057434\n",
      "Epoch: 185, loss: 0.017562778003301763\n",
      "Epoch: 190, loss: 0.01741993585576643\n",
      "Epoch: 195, loss: 0.01728289586961399\n",
      "Epoch: 200, loss: 0.017151263211303152\n",
      "Epoch: 205, loss: 0.017024679568142763\n",
      "Epoch: 210, loss: 0.016902818938420015\n",
      "Epoch: 215, loss: 0.016785383996530356\n",
      "Epoch: 220, loss: 0.016672102943029544\n",
      "Epoch: 225, loss: 0.016562726765282813\n",
      "Epoch: 230, loss: 0.01645702684716346\n",
      "Epoch: 235, loss: 0.016354792876639858\n",
      "Epoch: 240, loss: 0.016255831008552254\n",
      "Epoch: 245, loss: 0.016159962246794397\n",
      "Epoch: 250, loss: 0.016067021015783542\n",
      "Epoch: 255, loss: 0.015976853895770107\n",
      "Epoch: 260, loss: 0.015889318500399454\n",
      "Epoch: 265, loss: 0.0158042824781465\n",
      "Epoch: 270, loss: 0.015721622621922712\n",
      "Epoch: 275, loss: 0.015641224073399757\n",
      "Epoch: 280, loss: 0.015562979610483092\n",
      "Epoch: 285, loss: 0.01548678900796365\n",
      "Epoch: 290, loss: 0.01541255846272425\n",
      "Epoch: 295, loss: 0.015340200076021084\n",
      "Epoch: 300, loss: 0.015269631386329068\n",
      "Epoch: 305, loss: 0.01520077494705902\n",
      "Epoch: 310, loss: 0.015133557944144692\n",
      "Epoch: 315, loss: 0.015067911849077352\n",
      "Epoch: 320, loss: 0.015003772103451675\n",
      "Epoch: 325, loss: 0.014941077831500161\n",
      "Epoch: 330, loss: 0.0148797715774567\n",
      "Epoch: 335, loss: 0.014819799064929109\n",
      "Epoch: 340, loss: 0.014761108975801199\n",
      "Epoch: 345, loss: 0.014703652746543157\n",
      "Epoch: 350, loss: 0.014647384380185497\n",
      "Epoch: 355, loss: 0.014592260272586016\n",
      "Epoch: 360, loss: 0.014538239051947503\n",
      "Epoch: 365, loss: 0.01448528143078418\n",
      "Epoch: 370, loss: 0.014433350069654837\n",
      "Epoch: 375, loss: 0.014382409451984034\n",
      "Epoch: 380, loss: 0.014332425769218038\n",
      "Epoch: 385, loss: 0.014283366815463156\n",
      "Epoch: 390, loss: 0.01423520189068435\n",
      "Epoch: 395, loss: 0.014187901711530712\n",
      "Epoch: 400, loss: 0.014141438328905569\n",
      "Epoch: 405, loss: 0.014095785051496423\n",
      "Epoch: 410, loss: 0.014050916374601272\n",
      "Epoch: 415, loss: 0.01400680791370873\n",
      "Epoch: 420, loss: 0.01396343634239776\n",
      "Epoch: 425, loss: 0.013920779334208301\n",
      "Epoch: 430, loss: 0.013878815508197528\n",
      "Epoch: 435, loss: 0.013837524377939668\n",
      "Epoch: 440, loss: 0.013796886303754372\n",
      "Epoch: 445, loss: 0.01375688244796537\n",
      "Epoch: 450, loss: 0.013717494733000843\n",
      "Epoch: 455, loss: 0.013678705802153551\n",
      "Epoch: 460, loss: 0.0136404989828243\n",
      "Epoch: 465, loss: 0.013602858252078485\n",
      "Epoch: 470, loss: 0.013565768204352196\n",
      "Epoch: 475, loss: 0.013529214021152732\n",
      "Epoch: 480, loss: 0.013493181442606902\n",
      "Epoch: 485, loss: 0.013457656740720443\n",
      "Epoch: 490, loss: 0.013422626694221217\n",
      "Epoch: 495, loss: 0.01338807856486864\n",
      "Epoch: 500, loss: 0.01335400007512119\n",
      "Epoch: 505, loss: 0.013320379387062592\n",
      "Epoch: 510, loss: 0.013287205082495625\n",
      "Epoch: 515, loss: 0.013254466144120392\n",
      "Epoch: 520, loss: 0.013222151937721024\n",
      "Epoch: 525, loss: 0.013190252195291453\n",
      "Epoch: 530, loss: 0.01315875699903719\n",
      "Epoch: 535, loss: 0.013127656766195577\n",
      "Epoch: 540, loss: 0.013096942234622383\n",
      "Epoch: 545, loss: 0.01306660444909731\n",
      "Epoch: 550, loss: 0.013036634748305444\n",
      "Epoch: 555, loss: 0.013007024752455493\n",
      "Epoch: 560, loss: 0.012977766351499215\n",
      "Epoch: 565, loss: 0.012948851693919643\n",
      "Epoch: 570, loss: 0.01292027317605813\n",
      "Epoch: 575, loss: 0.012892023431952693\n",
      "Epoch: 580, loss: 0.012864095323662146\n",
      "Epoch: 585, loss: 0.01283648193205206\n",
      "Epoch: 590, loss: 0.012809176548020055\n",
      "Epoch: 595, loss: 0.01278217266413912\n",
      "Epoch: 600, loss: 0.012755463966698913\n",
      "Epoch: 605, loss: 0.012729044328125577\n",
      "Epoch: 610, loss: 0.012702907799761848\n",
      "Epoch: 615, loss: 0.012677048604989872\n",
      "Epoch: 620, loss: 0.012651461132679648\n",
      "Epoch: 625, loss: 0.01262613993094715\n",
      "Epoch: 630, loss: 0.012601079701206344\n",
      "Epoch: 635, loss: 0.01257627529250014\n",
      "Epoch: 640, loss: 0.01255172169609581\n",
      "Epoch: 645, loss: 0.012527414040330694\n",
      "Epoch: 650, loss: 0.012503347585694933\n",
      "Epoch: 655, loss: 0.012479517720137808\n",
      "Epoch: 660, loss: 0.012455919954585335\n",
      "Epoch: 665, loss: 0.012432549918656659\n",
      "Epoch: 670, loss: 0.012409403356567898\n",
      "Epoch: 675, loss: 0.01238647612321217\n",
      "Epoch: 680, loss: 0.012363764180405483\n",
      "Epoch: 685, loss: 0.01234126359328862\n",
      "Epoch: 690, loss: 0.012318970526876312\n",
      "Epoch: 695, loss: 0.012296881242745257\n",
      "Epoch: 700, loss: 0.012274992095854012\n",
      "Epoch: 705, loss: 0.012253299531487979\n",
      "Epoch: 710, loss: 0.012231800082324057\n",
      "Epoch: 715, loss: 0.012210490365609934\n",
      "Epoch: 720, loss: 0.012189367080453564\n",
      "Epoch: 725, loss: 0.012168427005219177\n",
      "Epoch: 730, loss: 0.012147666995025987\n",
      "Epoch: 735, loss: 0.012127083979346432\n",
      "Epoch: 740, loss: 0.012106674959700562\n",
      "Epoch: 745, loss: 0.012086437007443194\n",
      "Epoch: 750, loss: 0.012066367261640398\n",
      "Epoch: 755, loss: 0.012046462927031695\n",
      "Epoch: 760, loss: 0.012026721272074237\n",
      "Epoch: 765, loss: 0.012007139627065113\n",
      "Epoch: 770, loss: 0.011987715382337913\n",
      "Epoch: 775, loss: 0.011968445986529599\n",
      "Epoch: 780, loss: 0.011949328944913832\n",
      "Epoch: 785, loss: 0.011930361817797093\n",
      "Epoch: 790, loss: 0.011911542218973909\n",
      "Epoch: 795, loss: 0.011892867814237958\n",
      "Epoch: 800, loss: 0.011874336319945781\n",
      "Epoch: 805, loss: 0.011855945501630239\n",
      "Epoch: 810, loss: 0.011837693172661104\n",
      "Epoch: 815, loss: 0.011819577192950268\n",
      "Epoch: 820, loss: 0.01180159546769948\n",
      "Epoch: 825, loss: 0.01178374594618854\n",
      "Epoch: 830, loss: 0.01176602662060234\n",
      "Epoch: 835, loss: 0.01174843552489509\n",
      "Epoch: 840, loss: 0.011730970733690338\n",
      "Epoch: 845, loss: 0.01171363036121558\n",
      "Epoch: 850, loss: 0.011696412560270415\n",
      "Epoch: 855, loss: 0.011679315521227076\n",
      "Epoch: 860, loss: 0.011662337471062593\n",
      "Epoch: 865, loss: 0.011645476672421533\n",
      "Epoch: 870, loss: 0.011628731422708614\n",
      "Epoch: 875, loss: 0.011612100053210204\n",
      "Epoch: 880, loss: 0.011595580928243822\n",
      "Epoch: 885, loss: 0.011579172444334785\n",
      "Epoch: 890, loss: 0.011562873029418797\n",
      "Epoch: 895, loss: 0.011546681142069574\n",
      "Epoch: 900, loss: 0.01153059527075027\n",
      "Epoch: 905, loss: 0.011514613933087552\n",
      "Epoch: 910, loss: 0.011498735675167144\n",
      "Epoch: 915, loss: 0.011482959070849695\n",
      "Epoch: 920, loss: 0.011467282721105915\n",
      "Epoch: 925, loss: 0.011451705253369955\n",
      "Epoch: 930, loss: 0.011436225320910362\n",
      "Epoch: 935, loss: 0.01142084160221787\n",
      "Epoch: 940, loss: 0.01140555280040967\n",
      "Epoch: 945, loss: 0.011390357642649871\n",
      "Epoch: 950, loss: 0.011375254879585999\n",
      "Epoch: 955, loss: 0.011360243284801496\n",
      "Epoch: 960, loss: 0.011345321654283943\n",
      "Epoch: 965, loss: 0.011330488805908917\n",
      "Epoch: 970, loss: 0.01131574357893876\n",
      "Epoch: 975, loss: 0.011301084833535518\n",
      "Epoch: 980, loss: 0.011286511450286573\n",
      "Epoch: 985, loss: 0.01127202232974149\n",
      "Epoch: 990, loss: 0.011257616391957772\n",
      "Epoch: 995, loss: 0.01124329257605338\n",
      "Epoch: 1000, loss: 0.011229049839763763\n",
      "Epoch: 1005, loss: 0.01121488715900132\n",
      "Epoch: 1010, loss: 0.011200803527415985\n",
      "Epoch: 1015, loss: 0.011186797955956498\n",
      "Epoch: 1020, loss: 0.011172869472433105\n",
      "Epoch: 1025, loss: 0.011159017121083546\n",
      "Epoch: 1030, loss: 0.011145239962145843\n",
      "Epoch: 1035, loss: 0.01113153707144172\n",
      "Epoch: 1040, loss: 0.011117907539975783\n",
      "Epoch: 1045, loss: 0.011104350473554963\n",
      "Epoch: 1050, loss: 0.011090864992432638\n",
      "Epoch: 1055, loss: 0.01107745023098037\n",
      "Epoch: 1060, loss: 0.011064105337389116\n",
      "Epoch: 1065, loss: 0.011050829473399654\n",
      "Epoch: 1070, loss: 0.011037621814060684\n",
      "Epoch: 1075, loss: 0.011024481547511527\n",
      "Epoch: 1080, loss: 0.011011407874785422\n",
      "Epoch: 1085, loss: 0.010998400009628897\n",
      "Epoch: 1090, loss: 0.010985457178332601\n",
      "Epoch: 1095, loss: 0.010972578619569507\n",
      "Epoch: 1100, loss: 0.010959763584236785\n",
      "Epoch: 1105, loss: 0.010947011335298592\n",
      "Epoch: 1110, loss: 0.010934321147627437\n",
      "Epoch: 1115, loss: 0.01092169230784271\n",
      "Epoch: 1120, loss: 0.010909124114145014\n",
      "Epoch: 1125, loss: 0.010896615876145346\n",
      "Epoch: 1130, loss: 0.010884166914688345\n",
      "Epoch: 1135, loss: 0.010871776561668585\n",
      "Epoch: 1140, loss: 0.01085944415983923\n",
      "Epoch: 1145, loss: 0.01084716906261228\n",
      "Epoch: 1150, loss: 0.010834950633849894\n",
      "Epoch: 1155, loss: 0.010822788247646573\n",
      "Epoch: 1160, loss: 0.0108106812881024\n",
      "Epoch: 1165, loss: 0.010798629149088064\n",
      "Epoch: 1170, loss: 0.010786631234002734\n",
      "Epoch: 1175, loss: 0.010774686955526526\n",
      "Epoch: 1180, loss: 0.010762795735369668\n",
      "Epoch: 1185, loss: 0.010750957004020588\n",
      "Epoch: 1190, loss: 0.010739170200495743\n",
      "Epoch: 1195, loss: 0.01072743477209365\n",
      "Epoch: 1200, loss: 0.010715750174155743\n",
      "Epoch: 1205, loss: 0.010704115869836384\n",
      "Epoch: 1210, loss: 0.01069253132988397\n",
      "Epoch: 1215, loss: 0.010680996032434763\n",
      "Epoch: 1220, loss: 0.010669509462820493\n",
      "Epoch: 1225, loss: 0.010658071113390314\n",
      "Epoch: 1230, loss: 0.010646680483347321\n",
      "Epoch: 1235, loss: 0.010635337078599186\n",
      "Epoch: 1240, loss: 0.010624040411622297\n",
      "Epoch: 1245, loss: 0.010612790001338383\n",
      "Epoch: 1250, loss: 0.010601585373002416\n",
      "Epoch: 1255, loss: 0.010590426058100422\n",
      "Epoch: 1260, loss: 0.010579311594255875\n",
      "Epoch: 1265, loss: 0.010568241525143207\n",
      "Epoch: 1270, loss: 0.010557215400407247\n",
      "Epoch: 1275, loss: 0.010546232775587286\n",
      "Epoch: 1280, loss: 0.010535293212044822\n",
      "Epoch: 1285, loss: 0.010524396276893997\n",
      "Epoch: 1290, loss: 0.010513541542934098\n",
      "Epoch: 1295, loss: 0.010502728588583476\n",
      "Epoch: 1300, loss: 0.010491956997814514\n",
      "Epoch: 1305, loss: 0.010481226360089276\n",
      "Epoch: 1310, loss: 0.010470536270295832\n",
      "Epoch: 1315, loss: 0.01045988632868494\n",
      "Epoch: 1320, loss: 0.010449276140807349\n",
      "Epoch: 1325, loss: 0.010438705317451592\n",
      "Epoch: 1330, loss: 0.010428173474582511\n",
      "Epoch: 1335, loss: 0.010417680233280583\n",
      "Epoch: 1340, loss: 0.010407225219682167\n",
      "Epoch: 1345, loss: 0.010396808064920942\n",
      "Epoch: 1350, loss: 0.01038642840507048\n",
      "Epoch: 1355, loss: 0.010376085881088183\n",
      "Epoch: 1360, loss: 0.010365780138760492\n",
      "Epoch: 1365, loss: 0.010355510828649533\n",
      "Epoch: 1370, loss: 0.010345277606040894\n",
      "Epoch: 1375, loss: 0.010335080130892588\n",
      "Epoch: 1380, loss: 0.010324918067784801\n",
      "Epoch: 1385, loss: 0.010314791085870318\n",
      "Epoch: 1390, loss: 0.010304698858825068\n",
      "Epoch: 1395, loss: 0.010294641064798582\n",
      "Epoch: 1400, loss: 0.010284617386363697\n",
      "Epoch: 1405, loss: 0.010274627510465267\n",
      "Epoch: 1410, loss: 0.010264671128367283\n",
      "Epoch: 1415, loss: 0.010254747935598035\n",
      "Epoch: 1420, loss: 0.010244857631892998\n",
      "Epoch: 1425, loss: 0.010234999921135194\n",
      "Epoch: 1430, loss: 0.010225174511292899\n",
      "Epoch: 1435, loss: 0.01021538111435473\n",
      "Epoch: 1440, loss: 0.010205619446262236\n",
      "Epoch: 1445, loss: 0.01019588922684041\n",
      "Epoch: 1450, loss: 0.010186190179726357\n",
      "Epoch: 1455, loss: 0.010176522032296747\n",
      "Epoch: 1460, loss: 0.01016688451559463\n",
      "Epoch: 1465, loss: 0.010157277364255937\n",
      "Epoch: 1470, loss: 0.010147700316436392\n",
      "Epoch: 1475, loss: 0.010138153113739046\n",
      "Epoch: 1480, loss: 0.010128635501142678\n",
      "Epoch: 1485, loss: 0.010119147226931151\n",
      "Epoch: 1490, loss: 0.010109688042623594\n",
      "Epoch: 1495, loss: 0.010100257702905164\n",
      "Epoch: 1500, loss: 0.010090855965558048\n",
      "Epoch: 1505, loss: 0.010081482591392276\n",
      "Epoch: 1510, loss: 0.010072137344176089\n",
      "Epoch: 1515, loss: 0.010062819990565689\n",
      "Epoch: 1520, loss: 0.010053530300034502\n",
      "Epoch: 1525, loss: 0.010044268044802384\n",
      "Epoch: 1530, loss: 0.010035032999765903\n",
      "Epoch: 1535, loss: 0.01002582494243117\n",
      "Epoch: 1540, loss: 0.010016643652851546\n",
      "Epoch: 1545, loss: 0.010007488913573113\n",
      "Epoch: 1550, loss: 0.009998360509591698\n",
      "Epoch: 1555, loss: 0.009989258228325814\n",
      "Epoch: 1560, loss: 0.009980181859610422\n",
      "Epoch: 1565, loss: 0.00997113119571717\n",
      "Epoch: 1570, loss: 0.00996210603140628\n",
      "Epoch: 1575, loss: 0.00995310616401542\n",
      "Epoch: 1580, loss: 0.009944131393589132\n",
      "Epoch: 1585, loss: 0.009935181523050488\n",
      "Epoch: 1590, loss: 0.009926256358412787\n",
      "Epoch: 1595, loss: 0.009917355709024056\n",
      "Epoch: 1600, loss: 0.009908479387830552\n",
      "Epoch: 1605, loss: 0.009899627211638071\n",
      "Epoch: 1610, loss: 0.009890799001342779\n",
      "Epoch: 1615, loss: 0.009881994582098494\n",
      "Epoch: 1620, loss: 0.009873213783386868\n",
      "Epoch: 1625, loss: 0.009864456438963208\n",
      "Epoch: 1630, loss: 0.009855722386664064\n",
      "Epoch: 1635, loss: 0.00984701146808245\n",
      "Epoch: 1640, loss: 0.009838323528138257\n",
      "Epoch: 1645, loss: 0.00982965841458993\n",
      "Epoch: 1650, loss: 0.0098210159775426\n",
      "Epoch: 1655, loss: 0.0098123960690053\n",
      "Epoch: 1660, loss: 0.009803798542535078\n",
      "Epoch: 1665, loss: 0.009795223252984644\n",
      "Epoch: 1670, loss: 0.00978667005634786\n",
      "Epoch: 1675, loss: 0.009778138809680547\n",
      "Epoch: 1680, loss: 0.009769629371065642\n",
      "Epoch: 1685, loss: 0.009761141599591724\n",
      "Epoch: 1690, loss: 0.009752675355320976\n",
      "Epoch: 1695, loss: 0.009744230499232141\n",
      "Epoch: 1700, loss: 0.009735806893134474\n",
      "Epoch: 1705, loss: 0.0097274043995568\n",
      "Epoch: 1710, loss: 0.00971902288162117\n",
      "Epoch: 1715, loss: 0.009710662202912897\n",
      "Epoch: 1720, loss: 0.00970232222735858\n",
      "Epoch: 1725, loss: 0.00969400281912186\n",
      "Epoch: 1730, loss: 0.009685703842523584\n",
      "Epoch: 1735, loss: 0.009677425161990097\n",
      "Epoch: 1740, loss: 0.009669166642030227\n",
      "Epoch: 1745, loss: 0.009660928147239204\n",
      "Epoch: 1750, loss: 0.00965270954232613\n",
      "Epoch: 1755, loss: 0.009644510692160315\n",
      "Epoch: 1760, loss: 0.009636331461831753\n",
      "Epoch: 1765, loss: 0.009628171716720714\n",
      "Epoch: 1770, loss: 0.009620031322572255\n",
      "Epoch: 1775, loss: 0.0096119101455717\n",
      "Epoch: 1780, loss: 0.009603808052418275\n",
      "Epoch: 1785, loss: 0.009595724910394595\n",
      "Epoch: 1790, loss: 0.0095876605874304\n",
      "Epoch: 1795, loss: 0.00957961495215986\n",
      "Epoch: 1800, loss: 0.009571587873971724\n",
      "Epoch: 1805, loss: 0.009563579223052596\n",
      "Epoch: 1810, loss: 0.009555588870423416\n",
      "Epoch: 1815, loss: 0.009547616687969674\n",
      "Epoch: 1820, loss: 0.009539662548465867\n",
      "Epoch: 1825, loss: 0.00953172632559488\n",
      "Epoch: 1830, loss: 0.009523807893962805\n",
      "Epoch: 1835, loss: 0.009515907129109848\n",
      "Epoch: 1840, loss: 0.009508023907517713\n",
      "Epoch: 1845, loss: 0.009500158106613957\n",
      "Epoch: 1850, loss: 0.009492309604773602\n",
      "Epoch: 1855, loss: 0.009484478281318296\n",
      "Epoch: 1860, loss: 0.009476664016513044\n",
      "Epoch: 1865, loss: 0.009468866691560789\n",
      "Epoch: 1870, loss: 0.009461086188594708\n",
      "Epoch: 1875, loss: 0.00945332239066839\n",
      "Epoch: 1880, loss: 0.009445575181743847\n",
      "Epoch: 1885, loss: 0.009437844446677496\n",
      "Epoch: 1890, loss: 0.009430130071204482\n",
      "Epoch: 1895, loss: 0.009422431941921668\n",
      "Epoch: 1900, loss: 0.009414749946270196\n",
      "Epoch: 1905, loss: 0.00940708397251883\n",
      "Epoch: 1910, loss: 0.009399433909749942\n",
      "Epoch: 1915, loss: 0.009391799647850693\n",
      "Epoch: 1920, loss: 0.009384181077512823\n",
      "Epoch: 1925, loss: 0.009376578090245632\n",
      "Epoch: 1930, loss: 0.009368990578407824\n",
      "Epoch: 1935, loss: 0.009361418435265242\n",
      "Epoch: 1940, loss: 0.009353861555082672\n",
      "Epoch: 1945, loss: 0.009346319833259023\n",
      "Epoch: 1950, loss: 0.009338793166515822\n",
      "Epoch: 1955, loss: 0.009331281453148692\n",
      "Epoch: 1960, loss: 0.009323784593350187\n",
      "Epoch: 1965, loss: 0.009316302489609235\n",
      "Epoch: 1970, loss: 0.009308835047186981\n",
      "Epoch: 1975, loss: 0.009301382174660955\n",
      "Epoch: 1980, loss: 0.00929394378451844\n",
      "Epoch: 1985, loss: 0.009286519793766997\n",
      "Epoch: 1990, loss: 0.009279110124515447\n",
      "Epoch: 1995, loss: 0.00927171470446543\n",
      "Epoch: 2000, loss: 0.009264333467244312\n",
      "Epoch: 2005, loss: 0.009256966352508533\n",
      "Epoch: 2010, loss: 0.00924961330575608\n",
      "Epoch: 2015, loss: 0.009242274277808778\n",
      "Epoch: 2020, loss: 0.00923494922395936\n",
      "Epoch: 2025, loss: 0.009227638102819749\n",
      "Epoch: 2030, loss: 0.009220340874948942\n",
      "Epoch: 2035, loss: 0.00921305750137214\n",
      "Epoch: 2040, loss: 0.009205787942119858\n",
      "Epoch: 2045, loss: 0.009198532154912651\n",
      "Epoch: 2050, loss: 0.00919129009409424\n",
      "Epoch: 2055, loss: 0.009184061709879445\n",
      "Epoch: 2060, loss: 0.00917684694794047\n",
      "Epoch: 2065, loss: 0.009169645749315865\n",
      "Epoch: 2070, loss: 0.009162458050595608\n",
      "Epoch: 2075, loss: 0.009155283784317545\n",
      "Epoch: 2080, loss: 0.009148122879504532\n",
      "Epoch: 2085, loss: 0.009140975262275306\n",
      "Epoch: 2090, loss: 0.009133840856472843\n",
      "Epoch: 2095, loss: 0.009126719584267234\n",
      "Epoch: 2100, loss: 0.009119611366703907\n",
      "Epoch: 2105, loss: 0.009112516124180594\n",
      "Epoch: 2110, loss: 0.009105433776846028\n",
      "Epoch: 2115, loss: 0.009098364244920823\n",
      "Epoch: 2120, loss: 0.009091307448945698\n",
      "Epoch: 2125, loss: 0.009084263309964974\n",
      "Epoch: 2130, loss: 0.009077231749654498\n",
      "Epoch: 2135, loss: 0.009070212690403335\n",
      "Epoch: 2140, loss: 0.009063206055357972\n",
      "Epoch: 2145, loss: 0.009056211768436951\n",
      "Epoch: 2150, loss: 0.009049229754322577\n",
      "Epoch: 2155, loss: 0.009042259938435446\n",
      "Epoch: 2160, loss: 0.009035302246896188\n",
      "Epoch: 2165, loss: 0.009028356606478072\n",
      "Epoch: 2170, loss: 0.009021422944553124\n",
      "Epoch: 2175, loss: 0.009014501189033918\n",
      "Epoch: 2180, loss: 0.009007591268312425\n",
      "Epoch: 2185, loss: 0.009000693111197229\n",
      "Epoch: 2190, loss: 0.008993806646850013\n",
      "Epoch: 2195, loss: 0.008986931804722298\n",
      "Epoch: 2200, loss: 0.008980068514493601\n",
      "Epoch: 2205, loss: 0.008973216706012482\n",
      "Epoch: 2210, loss: 0.008966376309242791\n",
      "Epoch: 2215, loss: 0.008959547254218197\n",
      "Epoch: 2220, loss: 0.008952729471009653\n",
      "Epoch: 2225, loss: 0.00894592288971213\n",
      "Epoch: 2230, loss: 0.008939127440459478\n",
      "Epoch: 2235, loss: 0.008932343053478854\n",
      "Epoch: 2240, loss: 0.008925569659199576\n",
      "Epoch: 2245, loss: 0.008918807188434002\n",
      "Epoch: 2250, loss: 0.008912055572650533\n",
      "Epoch: 2255, loss: 0.008905314744358244\n",
      "Epoch: 2260, loss: 0.00889858463761854\n",
      "Epoch: 2265, loss: 0.008891865188687892\n",
      "Epoch: 2270, loss: 0.008885156336775318\n",
      "Epoch: 2275, loss: 0.008878458024868696\n",
      "Epoch: 2280, loss: 0.00887177020054707\n",
      "Epoch: 2285, loss: 0.008865092816662043\n",
      "Epoch: 2290, loss: 0.008858425831752944\n",
      "Epoch: 2295, loss: 0.008851769210074842\n",
      "Epoch: 2300, loss: 0.008845122921175244\n",
      "Epoch: 2305, loss: 0.008838486939047213\n",
      "Epoch: 2310, loss: 0.008831861240988002\n",
      "Epoch: 2315, loss: 0.00882524580636444\n",
      "Epoch: 2320, loss: 0.008818640615499066\n",
      "Epoch: 2325, loss: 0.008812045648838952\n",
      "Epoch: 2330, loss: 0.008805460886475044\n",
      "Epoch: 2335, loss: 0.008798886307982097\n",
      "Epoch: 2340, loss: 0.00879232189248052\n",
      "Epoch: 2345, loss: 0.008785767618795523\n",
      "Epoch: 2350, loss: 0.008779223465602145\n",
      "Epoch: 2355, loss: 0.008772689411480912\n",
      "Epoch: 2360, loss: 0.008766165434851644\n",
      "Epoch: 2365, loss: 0.008759651513790304\n",
      "Epoch: 2370, loss: 0.008753147625758946\n",
      "Epoch: 2375, loss: 0.00874665374729156\n",
      "Epoch: 2380, loss: 0.008740169853679772\n",
      "Epoch: 2385, loss: 0.00873369591869642\n",
      "Epoch: 2390, loss: 0.00872723191438371\n",
      "Epoch: 2395, loss: 0.008720777810920986\n",
      "Epoch: 2400, loss: 0.008714333576575707\n",
      "Epoch: 2405, loss: 0.008707899177732037\n",
      "Epoch: 2410, loss: 0.008701474578985485\n",
      "Epoch: 2415, loss: 0.008695059743288451\n",
      "Epoch: 2420, loss: 0.008688654632130813\n",
      "Epoch: 2425, loss: 0.008682259205740478\n",
      "Epoch: 2430, loss: 0.00867587342329102\n",
      "Epoch: 2435, loss: 0.008669497243106105\n",
      "Epoch: 2440, loss: 0.00866313062285326\n",
      "Epoch: 2445, loss: 0.008656773519722064\n",
      "Epoch: 2450, loss: 0.008650425890583935\n",
      "Epoch: 2455, loss: 0.008644087692132594\n",
      "Epoch: 2460, loss: 0.008637758881005273\n",
      "Epoch: 2465, loss: 0.00863143941388578\n",
      "Epoch: 2470, loss: 0.008625129247590908\n",
      "Epoch: 2475, loss: 0.008618828339141968\n",
      "Epoch: 2480, loss: 0.008612536645823347\n",
      "Epoch: 2485, loss: 0.008606254125229768\n",
      "Epoch: 2490, loss: 0.00859998073530394\n",
      "Epoch: 2495, loss: 0.008593716434366133\n",
      "Epoch: 2500, loss: 0.008587461181136895\n",
      "Epoch: 2505, loss: 0.00858121493475405\n",
      "Epoch: 2510, loss: 0.00857497765478499\n",
      "Epoch: 2515, loss: 0.008568749301234982\n",
      "Epoch: 2520, loss: 0.008562529834552272\n",
      "Epoch: 2525, loss: 0.008556319215630524\n",
      "Epoch: 2530, loss: 0.008550117405809066\n",
      "Epoch: 2535, loss: 0.008543924366871422\n",
      "Epoch: 2540, loss: 0.0085377400610424\n",
      "Epoch: 2545, loss: 0.008531564450984144\n",
      "Epoch: 2550, loss: 0.008525397499791278\n",
      "Epoch: 2555, loss: 0.008519239170985436\n",
      "Epoch: 2560, loss: 0.008513089428509339\n",
      "Epoch: 2565, loss: 0.008506948236720505\n",
      "Epoch: 2570, loss: 0.008500815560384792\n",
      "Epoch: 2575, loss: 0.008494691364669703\n",
      "Epoch: 2580, loss: 0.008488575615137706\n",
      "Epoch: 2585, loss: 0.008482468277739384\n",
      "Epoch: 2590, loss: 0.008476369318806527\n",
      "Epoch: 2595, loss: 0.008470278705045199\n",
      "Epoch: 2600, loss: 0.008464196403528544\n",
      "Epoch: 2605, loss: 0.008458122381689512\n",
      "Epoch: 2610, loss: 0.008452056607313277\n",
      "Epoch: 2615, loss: 0.00844599904852929\n",
      "Epoch: 2620, loss: 0.008439949673803066\n",
      "Epoch: 2625, loss: 0.008433908451927333\n",
      "Epoch: 2630, loss: 0.008427875352012807\n",
      "Epoch: 2635, loss: 0.00842185034347831\n",
      "Epoch: 2640, loss: 0.008415833396040339\n",
      "Epoch: 2645, loss: 0.008409824479702017\n",
      "Epoch: 2650, loss: 0.008403823564741512\n",
      "Epoch: 2655, loss: 0.008397830621699857\n",
      "Epoch: 2660, loss: 0.008391845621368379\n",
      "Epoch: 2665, loss: 0.008385868534775777\n",
      "Epoch: 2670, loss: 0.008379899333174888\n",
      "Epoch: 2675, loss: 0.008373937988029432\n",
      "Epoch: 2680, loss: 0.008367984471000742\n",
      "Epoch: 2685, loss: 0.008362038753934686\n",
      "Epoch: 2690, loss: 0.008356100808848974\n",
      "Epoch: 2695, loss: 0.008350170607920869\n",
      "Epoch: 2700, loss: 0.00834424812347559\n",
      "Epoch: 2705, loss: 0.008338333327975369\n",
      "Epoch: 2710, loss: 0.008332426194009382\n",
      "Epoch: 2715, loss: 0.008326526694284523\n",
      "Epoch: 2720, loss: 0.008320634801617148\n",
      "Epoch: 2725, loss: 0.008314750488925765\n",
      "Epoch: 2730, loss: 0.00830887372922471\n",
      "Epoch: 2735, loss: 0.008303004495618745\n",
      "Epoch: 2740, loss: 0.008297142761298627\n",
      "Epoch: 2745, loss: 0.008291288499537476\n",
      "Epoch: 2750, loss: 0.008285441683688086\n",
      "Epoch: 2755, loss: 0.008279602287180905\n",
      "Epoch: 2760, loss: 0.008273770283522783\n",
      "Epoch: 2765, loss: 0.00826794564629639\n",
      "Epoch: 2770, loss: 0.008262128349160193\n",
      "Epoch: 2775, loss: 0.008256318365849061\n",
      "Epoch: 2780, loss: 0.008250515670175332\n",
      "Epoch: 2785, loss: 0.00824472023603046\n",
      "Epoch: 2790, loss: 0.00823893203738708\n",
      "Epoch: 2795, loss: 0.00823315104830163\n",
      "Epoch: 2800, loss: 0.00822737724291742\n",
      "Epoch: 2805, loss: 0.00822161059546826\n",
      "Epoch: 2810, loss: 0.008215851080282683\n",
      "Epoch: 2815, loss: 0.008210098671788695\n",
      "Epoch: 2820, loss: 0.008204353344519364\n",
      "Epoch: 2825, loss: 0.008198615073119\n",
      "Epoch: 2830, loss: 0.008192883832350341\n",
      "Epoch: 2835, loss: 0.008187159597102618\n",
      "Epoch: 2840, loss: 0.00818144234240071\n",
      "Epoch: 2845, loss: 0.008175732043415495\n",
      "Epoch: 2850, loss: 0.008170028675475529\n",
      "Epoch: 2855, loss: 0.00816433221408016\n",
      "Epoch: 2860, loss: 0.008158642634914214\n",
      "Epoch: 2865, loss: 0.008152959913864454\n",
      "Epoch: 2870, loss: 0.008147284027037745\n",
      "Epoch: 2875, loss: 0.008141614950781225\n",
      "Epoch: 2880, loss: 0.008135952661704415\n",
      "Epoch: 2885, loss: 0.008130297136703353\n",
      "Epoch: 2890, loss: 0.008124648352986723\n",
      "Epoch: 2895, loss: 0.008119006288103896\n",
      "Epoch: 2900, loss: 0.008113370919974758\n",
      "Epoch: 2905, loss: 0.00810774222692105\n",
      "Epoch: 2910, loss: 0.008102120187698921\n",
      "Epoch: 2915, loss: 0.008096504781532225\n",
      "Epoch: 2920, loss: 0.008090895988146043\n",
      "Epoch: 2925, loss: 0.008085293787799642\n",
      "Epoch: 2930, loss: 0.008079698161318222\n",
      "Epoch: 2935, loss: 0.008074109090122395\n",
      "Epoch: 2940, loss: 0.008068526556254441\n",
      "Epoch: 2945, loss: 0.008062950542400325\n",
      "Epoch: 2950, loss: 0.008057381031906223\n",
      "Epoch: 2955, loss: 0.00805181800878867\n",
      "Epoch: 2960, loss: 0.008046261457737073\n",
      "Epoch: 2965, loss: 0.008040711364107988\n",
      "Epoch: 2970, loss: 0.00803516771391014\n",
      "Epoch: 2975, loss: 0.008029630493780126\n",
      "Epoch: 2980, loss: 0.008024099690948253\n",
      "Epoch: 2985, loss: 0.008018575293195081\n",
      "Epoch: 2990, loss: 0.008013057288798977\n",
      "Epoch: 2995, loss: 0.008007545666475753\n",
      "Epoch: 3000, loss: 0.008002040415311523\n",
      "Epoch: 3005, loss: 0.00799654152469047\n",
      "Epoch: 3010, loss: 0.007991048984219117\n",
      "Epoch: 3015, loss: 0.007985562783649102\n",
      "Epoch: 3020, loss: 0.00798008291280041\n",
      "Epoch: 3025, loss: 0.007974609361486877\n",
      "Epoch: 3030, loss: 0.007969142119445747\n",
      "Epoch: 3035, loss: 0.007963681176272865\n",
      "Epoch: 3040, loss: 0.007958226521364678\n",
      "Epoch: 3045, loss: 0.007952778143868041\n",
      "Epoch: 3050, loss: 0.00794733603263842\n",
      "Epoch: 3055, loss: 0.007941900176206789\n",
      "Epoch: 3060, loss: 0.007936470562755116\n",
      "Epoch: 3065, loss: 0.00793104718010033\n",
      "Epoch: 3070, loss: 0.007925630015686002\n",
      "Epoch: 3075, loss: 0.007920219056581246\n",
      "Epoch: 3080, loss: 0.007914814289485962\n",
      "Epoch: 3085, loss: 0.007909415700741556\n",
      "Epoch: 3090, loss: 0.00790402327634624\n",
      "Epoch: 3095, loss: 0.007898637001974063\n",
      "Epoch: 3100, loss: 0.007893256862996776\n",
      "Epoch: 3105, loss: 0.007887882844507924\n",
      "Epoch: 3110, loss: 0.007882514931348292\n",
      "Epoch: 3115, loss: 0.007877153108132263\n",
      "Epoch: 3120, loss: 0.007871797359274582\n",
      "Epoch: 3125, loss: 0.007866447669017094\n",
      "Epoch: 3130, loss: 0.007861104021455139\n",
      "Epoch: 3135, loss: 0.007855766400563423\n",
      "Epoch: 3140, loss: 0.00785043479022118\n",
      "Epoch: 3145, loss: 0.007845109174236494\n",
      "Epoch: 3150, loss: 0.007839789536369733\n",
      "Epoch: 3155, loss: 0.007834475860356115\n",
      "Epoch: 3160, loss: 0.007829168129927386\n",
      "Epoch: 3165, loss: 0.007823866328832607\n",
      "Epoch: 3170, loss: 0.007818570440858189\n",
      "Epoch: 3175, loss: 0.007813280449847072\n",
      "Epoch: 3180, loss: 0.007807996339717283\n",
      "Epoch: 3185, loss: 0.007802718094479728\n",
      "Epoch: 3190, loss: 0.007797445698255372\n",
      "Epoch: 3195, loss: 0.00779217913529177\n",
      "Epoch: 3200, loss: 0.00778691838997891\n",
      "Epoch: 3205, loss: 0.007781663446864404\n",
      "Epoch: 3210, loss: 0.007776414290667948\n",
      "Epoch: 3215, loss: 0.007771170906294907\n",
      "Epoch: 3220, loss: 0.0077659332788490515\n",
      "Epoch: 3225, loss: 0.007760701393644201\n",
      "Epoch: 3230, loss: 0.007755475236214685\n",
      "Epoch: 3235, loss: 0.007750254792324507\n",
      "Epoch: 3240, loss: 0.0077450400479749364\n",
      "Epoch: 3245, loss: 0.007739830989410425\n",
      "Epoch: 3250, loss: 0.007734627603122729\n",
      "Epoch: 3255, loss: 0.007729429875852942\n",
      "Epoch: 3260, loss: 0.007724237794591461\n",
      "Epoch: 3265, loss: 0.007719051346575674\n",
      "Epoch: 3270, loss: 0.007713870519285375\n",
      "Epoch: 3275, loss: 0.00770869530043584\n",
      "Epoch: 3280, loss: 0.007703525677968672\n",
      "Epoch: 3285, loss: 0.007698361640040468\n",
      "Epoch: 3290, loss: 0.007693203175009488\n",
      "Epoch: 3295, loss: 0.007688050271420528\n",
      "Epoch: 3300, loss: 0.007682902917988356\n",
      "Epoch: 3305, loss: 0.007677761103579922\n",
      "Epoch: 3310, loss: 0.007672624817195821\n",
      "Epoch: 3315, loss: 0.007667494047951376\n",
      "Epoch: 3320, loss: 0.007662368785057711\n",
      "Epoch: 3325, loss: 0.007657249017803327\n",
      "Epoch: 3330, loss: 0.007652134735536491\n",
      "Epoch: 3335, loss: 0.007647025927648881\n",
      "Epoch: 3340, loss: 0.007641922583560795\n",
      "Epoch: 3345, loss: 0.0076368246927082115\n",
      "Epoch: 3350, loss: 0.007631732244531906\n",
      "Epoch: 3355, loss: 0.007626645228468821\n",
      "Epoch: 3360, loss: 0.007621563633945792\n",
      "Epoch: 3365, loss: 0.00761648745037552\n",
      "Epoch: 3370, loss: 0.00761141666715493\n",
      "Epoch: 3375, loss: 0.007606351273665694\n",
      "Epoch: 3380, loss: 0.007601291259276744\n",
      "Epoch: 3385, loss: 0.007596236613348632\n",
      "Epoch: 3390, loss: 0.0075911873252394796\n",
      "Epoch: 3395, loss: 0.007586143384312154\n",
      "Epoch: 3400, loss: 0.007581104779942477\n",
      "Epoch: 3405, loss: 0.007576071501528124\n",
      "Epoch: 3410, loss: 0.007571043538497843\n",
      "Epoch: 3415, loss: 0.007566020880320771\n",
      "Epoch: 3420, loss: 0.0075610035165154885\n",
      "Epoch: 3425, loss: 0.007555991436658554\n",
      "Epoch: 3430, loss: 0.007550984630392266\n",
      "Epoch: 3435, loss: 0.007545983087431377\n",
      "Epoch: 3440, loss: 0.007540986797568651\n",
      "Epoch: 3445, loss: 0.007535995750679008\n",
      "Epoch: 3450, loss: 0.007531009936722165\n",
      "Epoch: 3455, loss: 0.007526029345743737\n",
      "Epoch: 3460, loss: 0.007521053967874632\n",
      "Epoch: 3465, loss: 0.007516083793328908\n",
      "Epoch: 3470, loss: 0.007511118812399942\n",
      "Epoch: 3475, loss: 0.007506159015455164\n",
      "Epoch: 3480, loss: 0.007501204392929317\n",
      "Epoch: 3485, loss: 0.0074962549353165256\n",
      "Epoch: 3490, loss: 0.007491310633161242\n",
      "Epoch: 3495, loss: 0.007486371477048365\n",
      "Epoch: 3500, loss: 0.00748143745759262\n",
      "Epoch: 3505, loss: 0.007476508565427628\n",
      "Epoch: 3510, loss: 0.0074715847911947\n",
      "Epoch: 3515, loss: 0.007466666125531745\n",
      "Epoch: 3520, loss: 0.007461752559062434\n",
      "Epoch: 3525, loss: 0.0074568440823858766\n",
      "Epoch: 3530, loss: 0.007451940686067062\n",
      "Epoch: 3535, loss: 0.00744704236062814\n",
      "Epoch: 3540, loss: 0.007442149096540783\n",
      "Epoch: 3545, loss: 0.007437260884219772\n",
      "Epoch: 3550, loss: 0.007432377714017883\n",
      "Epoch: 3555, loss: 0.007427499576222123\n",
      "Epoch: 3560, loss: 0.007422626461051461\n",
      "Epoch: 3565, loss: 0.007417758358656008\n",
      "Epoch: 3570, loss: 0.007412895259117674\n",
      "Epoch: 3575, loss: 0.007408037152452272\n",
      "Epoch: 3580, loss: 0.007403184028613102\n",
      "Epoch: 3585, loss: 0.007398335877495789\n",
      "Epoch: 3590, loss: 0.007393492688944519\n",
      "Epoch: 3595, loss: 0.00738865445275943\n",
      "Epoch: 3600, loss: 0.007383821158705076\n",
      "Epoch: 3605, loss: 0.007378992796519909\n",
      "Epoch: 3610, loss: 0.0073741693559266136\n",
      "Epoch: 3615, loss: 0.007369350826643169\n",
      "Epoch: 3620, loss: 0.007364537198394434\n",
      "Epoch: 3625, loss: 0.007359728460924246\n",
      "Epoch: 3630, loss: 0.007354924604007712\n",
      "Epoch: 3635, loss: 0.0073501256174636695\n",
      "Epoch: 3640, loss: 0.007345331491167033\n",
      "Epoch: 3645, loss: 0.0073405422150610005\n",
      "Epoch: 3650, loss: 0.007335757779168808\n",
      "Epoch: 3655, loss: 0.007330978173605034\n",
      "Epoch: 3660, loss: 0.007326203388586191\n",
      "Epoch: 3665, loss: 0.0073214334144405405\n",
      "Epoch: 3670, loss: 0.007316668241616988\n",
      "Epoch: 3675, loss: 0.007311907860692966\n",
      "Epoch: 3680, loss: 0.007307152262381178\n",
      "Epoch: 3685, loss: 0.0073024014375352025\n",
      "Epoch: 3690, loss: 0.007297655377153851\n",
      "Epoch: 3695, loss: 0.00729291407238423\n",
      "Epoch: 3700, loss: 0.0072881775145235556\n",
      "Epoch: 3705, loss: 0.007283445695019606\n",
      "Epoch: 3710, loss: 0.007278718605469968\n",
      "Epoch: 3715, loss: 0.007273996237619933\n",
      "Epoch: 3720, loss: 0.007269278583359196\n",
      "Epoch: 3725, loss: 0.007264565634717375\n",
      "Epoch: 3730, loss: 0.007259857383858347\n",
      "Epoch: 3735, loss: 0.007255153823073562\n",
      "Epoch: 3740, loss: 0.007250454944774322\n",
      "Epoch: 3745, loss: 0.0072457607414831695\n",
      "Epoch: 3750, loss: 0.007241071205824433\n",
      "Epoch: 3755, loss: 0.007236386330514049\n",
      "Epoch: 3760, loss: 0.0072317061083488265\n",
      "Epoch: 3765, loss: 0.007227030532195116\n",
      "Epoch: 3770, loss: 0.007222359594977236\n",
      "Epoch: 3775, loss: 0.007217693289665586\n",
      "Epoch: 3780, loss: 0.007213031609264681\n",
      "Epoch: 3785, loss: 0.007208374546801277\n",
      "Epoch: 3790, loss: 0.007203722095312648\n",
      "Epoch: 3795, loss: 0.007199074247835245\n",
      "Epoch: 3800, loss: 0.007194430997393778\n",
      "Epoch: 3805, loss: 0.007189792336990994\n",
      "Epoch: 3810, loss: 0.0071851582595981135\n",
      "Epoch: 3815, loss: 0.007180528758146175\n",
      "Epoch: 3820, loss: 0.007175903825518325\n",
      "Epoch: 3825, loss: 0.007171283454543122\n",
      "Epoch: 3830, loss: 0.007166667637988967\n",
      "Epoch: 3835, loss: 0.007162056368559696\n",
      "Epoch: 3840, loss: 0.007157449638891344\n",
      "Epoch: 3845, loss: 0.007152847441550121\n",
      "Epoch: 3850, loss: 0.007148249769031554\n",
      "Epoch: 3855, loss: 0.007143656613760849\n",
      "Epoch: 3860, loss: 0.007139067968094358\n",
      "Epoch: 3865, loss: 0.007134483824322106\n",
      "Epoch: 3870, loss: 0.007129904174671399\n",
      "Epoch: 3875, loss: 0.00712532901131131\n",
      "Epoch: 3880, loss: 0.00712075832635808\n",
      "Epoch: 3885, loss: 0.0071161921118812616\n",
      "Epoch: 3890, loss: 0.007111630359910578\n",
      "Epoch: 3895, loss: 0.0071070730624433025\n",
      "Epoch: 3900, loss: 0.007102520211452211\n",
      "Epoch: 3905, loss: 0.007097971798893862\n",
      "Epoch: 3910, loss: 0.007093427816717153\n",
      "Epoch: 3915, loss: 0.007088888256872145\n",
      "Epoch: 3920, loss: 0.007084353111318874\n",
      "Epoch: 3925, loss: 0.007079822372036235\n",
      "Epoch: 3930, loss: 0.007075296031030722\n",
      "Epoch: 3935, loss: 0.007070774080345015\n",
      "Epoch: 3940, loss: 0.007066256512066232\n",
      "Epoch: 3945, loss: 0.007061743318333858\n",
      "Epoch: 3950, loss: 0.007057234491347193\n",
      "Epoch: 3955, loss: 0.0070527300233722496\n",
      "Epoch: 3960, loss: 0.0070482299067480625\n",
      "Epoch: 3965, loss: 0.007043734133892276\n",
      "Epoch: 3970, loss: 0.0070392426973060005\n",
      "Epoch: 3975, loss: 0.0070347555895778\n",
      "Epoch: 3980, loss: 0.0070302728033868455\n",
      "Epoch: 3985, loss: 0.007025794331505121\n",
      "Epoch: 3990, loss: 0.007021320166798683\n",
      "Epoch: 3995, loss: 0.007016850302227899\n",
      "Epoch: 4000, loss: 0.007012384730846752\n",
      "Epoch: 4005, loss: 0.00700792344580111\n",
      "Epoch: 4010, loss: 0.007003466440326042\n",
      "Epoch: 4015, loss: 0.006999013707742174\n",
      "Epoch: 4020, loss: 0.0069945652414511944\n",
      "Epoch: 4025, loss: 0.0069901210349305\n",
      "Epoch: 4030, loss: 0.006985681081727126\n",
      "Epoch: 4035, loss: 0.006981245375451005\n",
      "Epoch: 4040, loss: 0.006976813909767702\n",
      "Epoch: 4045, loss: 0.006972386678390728\n",
      "Epoch: 4050, loss: 0.006967963675073511\n",
      "Epoch: 4055, loss: 0.006963544893601262\n",
      "Epoch: 4060, loss: 0.006959130327782743\n",
      "Epoch: 4065, loss: 0.0069547199714421306\n",
      "Epoch: 4070, loss: 0.006950313818411143\n",
      "Epoch: 4075, loss: 0.006945911862521425\n",
      "Epoch: 4080, loss: 0.006941514097597429\n",
      "Epoch: 4085, loss: 0.006937120517449806\n",
      "Epoch: 4090, loss: 0.006932731115869398\n",
      "Epoch: 4095, loss: 0.006928345886621923\n",
      "Epoch: 4100, loss: 0.006923964823443363\n",
      "Epoch: 4105, loss: 0.006919587920036079\n",
      "Epoch: 4110, loss: 0.006915215170065716\n",
      "Epoch: 4115, loss: 0.00691084656715876\n",
      "Epoch: 4120, loss: 0.006906482104900894\n",
      "Epoch: 4125, loss: 0.006902121776835917\n",
      "Epoch: 4130, loss: 0.006897765576465328\n",
      "Epoch: 4135, loss: 0.006893413497248412\n",
      "Epoch: 4140, loss: 0.006889065532602771\n",
      "Epoch: 4145, loss: 0.006884721675905279\n",
      "Epoch: 4150, loss: 0.006880381920493289\n",
      "Epoch: 4155, loss: 0.0068760462596660875\n",
      "Epoch: 4160, loss: 0.00687171468668651\n",
      "Epoch: 4165, loss: 0.006867387194782626\n",
      "Epoch: 4170, loss: 0.00686306377714949\n",
      "Epoch: 4175, loss: 0.006858744426950875\n",
      "Epoch: 4180, loss: 0.006854429137320974\n",
      "Epoch: 4185, loss: 0.00685011790136604\n",
      "Epoch: 4190, loss: 0.006845810712166007\n",
      "Epoch: 4195, loss: 0.006841507562776002\n",
      "Epoch: 4200, loss: 0.006837208446227846\n",
      "Epoch: 4205, loss: 0.006832913355531517\n",
      "Epoch: 4210, loss: 0.006828622283676574\n",
      "Epoch: 4215, loss: 0.006824335223633661\n",
      "Epoch: 4220, loss: 0.006820052168355948\n",
      "Epoch: 4225, loss: 0.006815773110780725\n",
      "Epoch: 4230, loss: 0.006811498043831009\n",
      "Epoch: 4235, loss: 0.006807226960417259\n",
      "Epoch: 4240, loss: 0.006802959853439195\n",
      "Epoch: 4245, loss: 0.006798696715787722\n",
      "Epoch: 4250, loss: 0.006794437540346979\n",
      "Epoch: 4255, loss: 0.006790182319996463\n",
      "Epoch: 4260, loss: 0.0067859310476133\n",
      "Epoch: 4265, loss: 0.00678168371607458\n",
      "Epoch: 4270, loss: 0.006777440318259824\n",
      "Epoch: 4275, loss: 0.006773200847053501\n",
      "Epoch: 4280, loss: 0.006768965295347616\n",
      "Epoch: 4285, loss: 0.006764733656044413\n",
      "Epoch: 4290, loss: 0.006760505922059075\n",
      "Epoch: 4295, loss: 0.006756282086322526\n",
      "Epoch: 4300, loss: 0.0067520621417842515\n",
      "Epoch: 4305, loss: 0.006747846081415197\n",
      "Epoch: 4310, loss: 0.006743633898210683\n",
      "Epoch: 4315, loss: 0.006739425585193403\n",
      "Epoch: 4320, loss: 0.006735221135416459\n",
      "Epoch: 4325, loss: 0.006731020541966437\n",
      "Epoch: 4330, loss: 0.006726823797966597\n",
      "Epoch: 4335, loss: 0.006722630896580077\n",
      "Epoch: 4340, loss: 0.006718441831013211\n",
      "Epoch: 4345, loss: 0.006714256594518927\n",
      "Epoch: 4350, loss: 0.006710075180400196\n",
      "Epoch: 4355, loss: 0.006705897582013612\n",
      "Epoch: 4360, loss: 0.006701723792773056\n",
      "Epoch: 4365, loss: 0.0066975538061533986\n",
      "Epoch: 4370, loss: 0.0066933876156943465\n",
      "Epoch: 4375, loss: 0.006689225215004304\n",
      "Epoch: 4380, loss: 0.0066850665977643345\n",
      "Epoch: 4385, loss: 0.0066809117577321065\n",
      "Epoch: 4390, loss: 0.006676760688745862\n",
      "Epoch: 4395, loss: 0.006672613384728362\n",
      "Epoch: 4400, loss: 0.006668469839690735\n",
      "Epoch: 4405, loss: 0.006664330047736239\n",
      "Epoch: 4410, loss: 0.006660194003063807\n",
      "Epoch: 4415, loss: 0.006656061699971369\n",
      "Epoch: 4420, loss: 0.006651933132858897\n",
      "Epoch: 4425, loss: 0.006647808296231003\n",
      "Epoch: 4430, loss: 0.00664368718469917\n",
      "Epoch: 4435, loss: 0.00663956979298335\n",
      "Epoch: 4440, loss: 0.00663545611591304\n",
      "Epoch: 4445, loss: 0.006631346148427609\n",
      "Epoch: 4450, loss: 0.006627239885575869\n",
      "Epoch: 4455, loss: 0.006623137322514818\n",
      "Epoch: 4460, loss: 0.006619038454507531\n",
      "Epoch: 4465, loss: 0.006614943276920056\n",
      "Epoch: 4470, loss: 0.006610851785217407\n",
      "Epoch: 4475, loss: 0.0066067639749585755\n",
      "Epoch: 4480, loss: 0.006602679841790558\n",
      "Epoch: 4485, loss: 0.006598599381441535\n",
      "Epoch: 4490, loss: 0.006594522589713069\n",
      "Epoch: 4495, loss: 0.006590449462471612\n",
      "Epoch: 4500, loss: 0.006586379995639272\n",
      "Epoch: 4505, loss: 0.006582314185183971\n",
      "Epoch: 4510, loss: 0.006578252027109216\n",
      "Epoch: 4515, loss: 0.006574193517443511\n",
      "Epoch: 4520, loss: 0.0065701386522296545\n",
      "Epoch: 4525, loss: 0.006566087427514028\n",
      "Epoch: 4530, loss: 0.006562039839336071\n",
      "Epoch: 4535, loss: 0.006557995883718038\n",
      "Epoch: 4540, loss: 0.00655395555665526\n",
      "Epoch: 4545, loss: 0.00654991885410699\n",
      "Epoch: 4550, loss: 0.0065458857719879185\n",
      "Epoch: 4555, loss: 0.006541856306160536\n",
      "Epoch: 4560, loss: 0.006537830452428296\n",
      "Epoch: 4565, loss: 0.006533808206529786\n",
      "Epoch: 4570, loss: 0.006529789564133728\n",
      "Epoch: 4575, loss: 0.006525774520835005\n",
      "Epoch: 4580, loss: 0.00652176307215156\n",
      "Epoch: 4585, loss: 0.006517755213522222\n",
      "Epoch: 4590, loss: 0.0065137509403053375\n",
      "Epoch: 4595, loss: 0.006509750247778206\n",
      "Epoch: 4600, loss: 0.006505753131137199\n",
      "Epoch: 4605, loss: 0.006501759585498518\n",
      "Epoch: 4610, loss: 0.0064977696058994925\n",
      "Epoch: 4615, loss: 0.00649378318730032\n",
      "Epoch: 4620, loss: 0.006489800324586224\n",
      "Epoch: 4625, loss: 0.006485821012569858\n",
      "Epoch: 4630, loss: 0.006481845245993985\n",
      "Epoch: 4635, loss: 0.006477873019534268\n",
      "Epoch: 4640, loss: 0.0064739043278021794\n",
      "Epoch: 4645, loss: 0.006469939165347968\n",
      "Epoch: 4650, loss: 0.006465977526663577\n",
      "Epoch: 4655, loss: 0.006462019406185541\n",
      "Epoch: 4660, loss: 0.006458064798297815\n",
      "Epoch: 4665, loss: 0.0064541136973344845\n",
      "Epoch: 4670, loss: 0.006450166097582371\n",
      "Epoch: 4675, loss: 0.006446221993283489\n",
      "Epoch: 4680, loss: 0.006442281378637377\n",
      "Epoch: 4685, loss: 0.006438344247803289\n",
      "Epoch: 4690, loss: 0.006434410594902217\n",
      "Epoch: 4695, loss: 0.00643048041401883\n",
      "Epoch: 4700, loss: 0.0064265536992032584\n",
      "Epoch: 4705, loss: 0.006422630444472747\n",
      "Epoch: 4710, loss: 0.006418710643813254\n",
      "Epoch: 4715, loss: 0.006414794291180937\n",
      "Epoch: 4720, loss: 0.006410881380503579\n",
      "Epoch: 4725, loss: 0.006406971905681983\n",
      "Epoch: 4730, loss: 0.00640306586059134\n",
      "Epoch: 4735, loss: 0.0063991632390825995\n",
      "Epoch: 4740, loss: 0.006395264034983851\n",
      "Epoch: 4745, loss: 0.006391368242101811\n",
      "Epoch: 4750, loss: 0.006387475854223317\n",
      "Epoch: 4755, loss: 0.006383586865117029\n",
      "Epoch: 4760, loss: 0.006379701268535184\n",
      "Epoch: 4765, loss: 0.006375819058215606\n",
      "Epoch: 4770, loss: 0.0063719402278838875\n",
      "Epoch: 4775, loss: 0.006368064771255864\n",
      "Epoch: 4780, loss: 0.006364192682040368\n",
      "Epoch: 4785, loss: 0.006360323953942338\n",
      "Epoch: 4790, loss: 0.006356458580666314\n",
      "Epoch: 4795, loss: 0.006352596555920406\n",
      "Epoch: 4800, loss: 0.006348737873420733\n",
      "Epoch: 4805, loss: 0.006344882526896476\n",
      "Epoch: 4810, loss: 0.006341030510095519\n",
      "Epoch: 4815, loss: 0.006337181816790839\n",
      "Epoch: 4820, loss: 0.0063333364407876635\n",
      "Epoch: 4825, loss: 0.006329494375931517\n",
      "Epoch: 4830, loss: 0.006325655616117233\n",
      "Epoch: 4835, loss: 0.006321820155299008\n",
      "Epoch: 4840, loss: 0.006317987987501666\n",
      "Epoch: 4845, loss: 0.00631415910683317\n",
      "Epoch: 4850, loss: 0.006310333507498503\n",
      "Epoch: 4855, loss: 0.006306511183815034\n",
      "Epoch: 4860, loss: 0.006302692130229415\n",
      "Epoch: 4865, loss: 0.006298876341336067\n",
      "Epoch: 4870, loss: 0.006295063811897255\n",
      "Epoch: 4875, loss: 0.00629125453686467\n",
      "Epoch: 4880, loss: 0.006287448511402389\n",
      "Epoch: 4885, loss: 0.006283645730910913\n",
      "Epoch: 4890, loss: 0.006279846191051873\n",
      "Epoch: 4895, loss: 0.006276049887772777\n",
      "Epoch: 4900, loss: 0.00627225681733107\n",
      "Epoch: 4905, loss: 0.006268466976316467\n",
      "Epoch: 4910, loss: 0.0062646803616702995\n",
      "Epoch: 4915, loss: 0.006260896970700662\n",
      "Epoch: 4920, loss: 0.006257116801091635\n",
      "Epoch: 4925, loss: 0.006253339850905309\n",
      "Epoch: 4930, loss: 0.006249566118574977\n",
      "Epoch: 4935, loss: 0.0062457956028884225\n",
      "Epoch: 4940, loss: 0.006242028302960508\n",
      "Epoch: 4945, loss: 0.006238264218194753\n",
      "Epoch: 4950, loss: 0.006234503348234396\n",
      "Epoch: 4955, loss: 0.0062307456929040885\n",
      "Epoch: 4960, loss: 0.006226991252144046\n",
      "Epoch: 4965, loss: 0.006223240025939253\n",
      "Epoch: 4970, loss: 0.006219492014246621\n",
      "Epoch: 4975, loss: 0.006215747216923417\n",
      "Epoch: 4980, loss: 0.0062120056336600156\n",
      "Epoch: 4985, loss: 0.006208267263919954\n",
      "Epoch: 4990, loss: 0.006204532106889625\n",
      "Epoch: 4995, loss: 0.0062008001614392\n",
      "Last Epoch: 5000, loss: 0.006197816916438564\n"
     ]
    }
   ],
   "source": [
    "w_input, w_output, bias_input, bias_output = mlp_train(X_train, y_train, n_neurons, epochs=5000, l_rate=0.1, criteria=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cb33864-4d8c-459e-8c54-0b9eb2380d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,)\n",
      "(10000, 1)\n",
      "(10000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Função de predição do MLP\n",
    "def mlp_predict(X, w_in, w_out, bias_in, bias_out):\n",
    "    # Forward\n",
    "    Zin = (X @ w_in) + bias_in.T\n",
    "    result_in = activate_functions('sigmoid', Zin)\n",
    "\n",
    "    Zout = (result_in @ w_out) + bias_out.T\n",
    "    result_out = activate_functions('softmax', Zout)\n",
    "\n",
    "    # Converte as saídas para as classes preditas (0 a 9) usando a função argmax\n",
    "    # A classe predita será o índice do valor máximo em cada linha\n",
    "    classe = np.argmax(result_out, axis=1)\n",
    "    print(classe.shape)\n",
    "\n",
    "    return np.expand_dims(classe, axis=1)\n",
    "\n",
    "# Realizar a predição no conjunto de teste\n",
    "y_pred_test = mlp_predict(X_test, w_input, w_output, bias_input, bias_output)\n",
    "\n",
    "print(y_pred_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf44b347-dfab-4dd4-a3c8-b4b59da9908c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate: 0.0488\n"
     ]
    }
   ],
   "source": [
    "# Avaliar o desempenho do classificador\n",
    "error = (10000 - sum(y_pred_test == y_test)) / 10000 \n",
    "print(\"Error rate: {}\".format(error[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4dda21-8ac4-4888-9805-0e12c3de1531",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": ".env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
